{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Machine Learning\n",
    "# Heterogeneous Stacked Ensemble Classifier - The Super Learner\n",
    "# Aida Sehic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages Etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML, Image\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import transpose\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import sympy \n",
    "\n",
    "\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn import ensemble\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import neural_network\n",
    "from sklearn import svm \n",
    "from sklearn.svm import SVC \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection._split import check_cv\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "import sklearn.preprocessing as skp\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "#%qtconsole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Super Learner Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *Super Learner* is a heterogeneous stacked ensemble classifier. This is a classification model that uses a set of base classifiers of different types, the outputs of which are then combined in another classifier at the stacked layer. The Super Learner was described in [(van der Laan et al, 2007)](https://pdfs.semanticscholar.org/4e81/f4c95dde9a327026c584b91a3fe691595d1f.pdf) but the stacked ensemble idea has been around for a long time. \n",
    "\n",
    "Figure 1 shows a flow diagram of the Super Learner process (this is from (van der Laan et al, 2007). The base classifiers are trained and their outputs are combined along with the training dataset labels into a training set for the stack layer classifier. To avoid overfitting the generation of the stacked layer training set uses a k-fold cross validation process (described as V-fold in Figure 1). To further add variety to the base estimators a bootstrapping selection (as is used in the bagging ensemble approach).\n",
    " \n",
    "![Super Learner Process Flow](SuperLearnerProcessFlow.png \"Logo Title Text 1\")\n",
    "Figure 1: A flow diagram for the Super Learner\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the SuperLearnerClassifier Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a new classifier which is based on the sckit-learn BaseEstimator and \n",
    "#ClassifierMixin classes\n",
    "class SuperLearnerClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \n",
    "    \"\"\"An ensemble classifier that uses heterogeneous models at the base layer \n",
    "    and a aggregatnio model at the aggregation layer. \n",
    "    A k-fold cross validation is used to gnerate training data for the \n",
    "    stack layer model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    base_classifiers: A list of base classifiers of different types.\n",
    "    \n",
    "    stacked_layer_classifier: The stacked layer classifier that will \n",
    "                               be trained on the outputs of the base classifiers\n",
    "    probability_outputs: bool (default: False)\n",
    "        If True, indicates that the stacked layer classifier should be trained on \n",
    "        probability outputs from the base classifiers. \n",
    "        If False (default) than train stacked layer classifier on label outputs \n",
    "        from the base classifiers\n",
    "        \n",
    "    type_of_model: selection of the stack layer classifier type \n",
    "        Stack layer model can be selected from a set of 10 algorithms from scikit-learn\n",
    "        by setting up the value of a type_of_model parametar to be in a range from 0 to 9\n",
    "        If type_of_model = i, where i in [0,9], then \n",
    "                    stacked_layer_classifier = clsf_type[i]\n",
    "        Note: clsf_type is the set of following 10 algorithms from scikit-learn:\n",
    "        clsf_type = [\n",
    "            DecisionTreeClassifier(criterion=\"entropy\", min_samples_split = 200, max_depth=6),\n",
    "            LogisticRegression(C=1.8, max_iter=1000, multi_class='ovr', solver= 'liblinear'),\n",
    "            svm.SVC(kernel='linear', C=100, probability=True),\n",
    "            KNeighborsClassifier(n_neighbors=6) ,\n",
    "            RandomForestClassifier(criterion='gini',max_depth=100,n_estimators=10),\n",
    "            GaussianNB(),\n",
    "            neural_network.MLPClassifier(hidden_layer_sizes=(400,200,100), alpha = 0.01),\n",
    "            ensemble.BaggingClassifier(base_estimator = DecisionTreeClassifier(criterion=\"entropy\", min_samples_split = 200, max_depth=6), bootstrap=True, max_samples= 1.0),\n",
    "            ensemble.AdaBoostClassifier(base_estimator = DecisionTreeClassifier(criterion=\"entropy\", min_samples_split = 200, max_depth=6), n_estimators=250),\n",
    "            GradientBoostingClassifier()\n",
    "        ]\n",
    "        \n",
    "    base_clsf_type: selection of base estimators types combination   \n",
    "        Base estimators types combination can be selected from a specific set of 45 algorithms combinations from scikit-learn\n",
    "        by setting up the value of a base_clsf_type parametar to be in a range from 0 to 44\n",
    "        If base_clsf_type = i, where i in [0,44], then \n",
    "                    base_classifiers = base_clfs_comb[i]\n",
    "        Note: base_clfs_comb is defined within the fit part:\n",
    "        \n",
    "        \n",
    "    add_original_input: bool (default: False)\n",
    "        If True, then training dataset for the stack layer classifier will contain\n",
    "        outputs/predictions of the base classifiers and the original input\n",
    "        If False, the stack layer classifier will be trained only on the outputs\n",
    "        of the base classifiers combined along with dataset labels.\n",
    "        \n",
    "    (sklearn.model_selection.check_cv(cv=3, y=None, classifier=False)[source] \n",
    "    --> http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.check_cv.html)    \n",
    "    cv: int, cross-validation generator or an iterable (default: 3)\n",
    "        Determines the cross-validation splitting strategy. Possible inputs for cv are:\n",
    "          -> None, to use the default 3-fold cross validation,\n",
    "          -> integer, to specify the number of folds.\n",
    "          -> An object to be used as a cross-validation generator.\n",
    "          -> An iterable yielding train, test splits.\n",
    "        For integer/None inputs, if classifier is True and \n",
    "        y is either binary or multiclass, StratifiedKFold is used. \n",
    "        In all other cases, KFold is used.\n",
    "    stratify: bool (default: True)\n",
    "        By default is True and it will follow a stratified K-Fold cross validation technique. \n",
    "        If the `cv` argument is a specific cross validation technique, this argument is omitted.\n",
    "    shuffle: bool (default: True)\n",
    "        If True, whether to shuffle (each stratification of) the data before splitting into batches.\n",
    "        \n",
    "    Attributes\n",
    "    ----------\n",
    "    \n",
    "\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    \n",
    "\n",
    "    See also\n",
    "    --------\n",
    "    \n",
    "    ----------\n",
    "    .. [1]  van der Laan, M., Polley, E. & Hubbard, A. (2007). \n",
    "            Super Learner. Statistical Applications in Genetics \n",
    "            and Molecular Biology, 6(1) \n",
    "            doi:10.2202/1544-6115.1309\n",
    "     \n",
    "    \"\"\"\n",
    "    # Constructor for the classifier object\n",
    "    def __init__(self, \n",
    "                 base_classifiers=[DecisionTreeClassifier(criterion=\"entropy\", min_samples_split = 200, max_depth=6),\n",
    "                                   LogisticRegression(C=1.8, max_iter=1000, multi_class='ovr', solver= 'liblinear'),\n",
    "                                   svm.SVC(kernel='linear', C=100, probability=True),\n",
    "                                   KNeighborsClassifier(n_neighbors=6),\n",
    "                                   RandomForestClassifier(criterion='gini',max_depth=100,n_estimators=10),\n",
    "                                   GaussianNB()] , #fixed set of 6 heterogeneous base models\n",
    "                 stacked_layer_classifier = DecisionTreeClassifier(criterion=\"entropy\", \n",
    "                                                                   min_samples_split = 200, \n",
    "                                                                   max_depth=6), # stacked layer model is by default decision tree\n",
    "                 type_of_model=0,\n",
    "                 base_clsf_type=0,\n",
    "                 cv=3,\n",
    "                 stratify=True,\n",
    "                 shuffle=True,\n",
    "                 probability_outputs=False,\n",
    "                 add_original_input=False):\n",
    "        \"\"\"Setup a SuperLearner classifier .\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "\n",
    "        \"\"\"  \n",
    "        self.base_classifiers = base_classifiers\n",
    "        self.stacked_layer_classifier = stacked_layer_classifier\n",
    "        \n",
    "        self.type_of_model=type_of_model\n",
    "        self.base_clsf_type=base_clsf_type\n",
    "        self.cv = cv\n",
    "        self.stratify = stratify\n",
    "        self.shuffle = shuffle\n",
    "        \n",
    "        self.probability_outputs = probability_outputs\n",
    "        self.add_original_input = add_original_input\n",
    "        #self.type_of_model = type_of_model\n",
    "        \n",
    "\n",
    "    # The fit function to train a classifier\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Build a SuperLearner classifier from the training set (X, y).\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape = [n_samples, n_features]\n",
    "            The training input samples. \n",
    "        y : array-like, shape = [n_samples] \n",
    "            The target values (class labels) as integers or strings.\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "        \"\"\"     \n",
    "        \n",
    "        self.clfs_ = self.base_classifiers\n",
    "        self.s_clf_ = self.stacked_layer_classifier \n",
    "        clsf_type = [\n",
    "            DecisionTreeClassifier(criterion=\"entropy\", min_samples_split = 200, max_depth=6),\n",
    "            LogisticRegression(C=1.8, max_iter=1000, multi_class='ovr', solver= 'liblinear'),\n",
    "            svm.SVC(kernel='linear', C=100, probability=True),\n",
    "            KNeighborsClassifier(n_neighbors=6) ,\n",
    "            RandomForestClassifier(criterion='gini',max_depth=100,n_estimators=10),\n",
    "            GaussianNB(),\n",
    "            neural_network.MLPClassifier(hidden_layer_sizes=(400,200,100), alpha = 0.01),\n",
    "            ensemble.BaggingClassifier(base_estimator = DecisionTreeClassifier(criterion=\"entropy\", min_samples_split = 200, max_depth=6), bootstrap=True, max_samples= 1.0),\n",
    "            ensemble.AdaBoostClassifier(base_estimator = DecisionTreeClassifier(criterion=\"entropy\", min_samples_split = 200, max_depth=6), n_estimators=250),\n",
    "            ensemble.GradientBoostingClassifier()\n",
    "        ]\n",
    "        \n",
    "        for i in range(10):    \n",
    "            if self.type_of_model==i:\n",
    "                self.s_clf_ = clsf_type[i]\n",
    "       \n",
    "            \n",
    "        #list of some of base classifiers types combinations (45 combination - doesn't include all 15 combinations for 2 types out of 6)    \n",
    "        base_clfs_comb = [\n",
    "            \n",
    "            [DecisionTreeClassifier(criterion=\"entropy\", min_samples_split = 200, max_depth=6),\n",
    "             LogisticRegression(C=1.8, max_iter=1000, multi_class='ovr', solver= 'liblinear'),\n",
    "             svm.SVC(kernel='linear', C=100, probability=True),\n",
    "             KNeighborsClassifier(n_neighbors=6),\n",
    "             RandomForestClassifier(criterion='gini',max_depth=100,n_estimators=10),\n",
    "             GaussianNB()],\n",
    "            \n",
    "            [DecisionTreeClassifier(criterion=\"entropy\", min_samples_split = 200, max_depth=6),\n",
    "             LogisticRegression(C=1.8, max_iter=1000, multi_class='ovr', solver= 'liblinear'),\n",
    "             svm.SVC(kernel='linear', C=100, probability=True),\n",
    "             KNeighborsClassifier(n_neighbors=6),\n",
    "             RandomForestClassifier(criterion='gini',max_depth=100,n_estimators=10)],\n",
    "            \n",
    "            [DecisionTreeClassifier(criterion=\"entropy\", min_samples_split = 200, max_depth=6),\n",
    "             LogisticRegression(C=1.8, max_iter=1000, multi_class='ovr', solver= 'liblinear'),\n",
    "             svm.SVC(kernel='linear', C=100, probability=True),\n",
    "             KNeighborsClassifier(n_neighbors=6),\n",
    "             GaussianNB()],\n",
    "            \n",
    "            [DecisionTreeClassifier(criterion=\"entropy\", min_samples_split = 200, max_depth=6),\n",
    "             LogisticRegression(C=1.8, max_iter=1000, multi_class='ovr', solver= 'liblinear'),\n",
    "             svm.SVC(kernel='linear', C=100, probability=True),\n",
    "             RandomForestClassifier(criterion='gini',max_depth=100,n_estimators=10),\n",
    "             GaussianNB()],\n",
    "            \n",
    "            [DecisionTreeClassifier(criterion=\"entropy\", min_samples_split = 200, max_depth=6),\n",
    "             LogisticRegression(C=1.8, max_iter=1000, multi_class='ovr', solver= 'liblinear'),\n",
    "             KNeighborsClassifier(n_neighbors=6),\n",
    "             RandomForestClassifier(criterion='gini',max_depth=100,n_estimators=10),\n",
    "             GaussianNB()],  \n",
    "            \n",
    "            [DecisionTreeClassifier(criterion=\"entropy\", min_samples_split = 200, max_depth=6),\n",
    "             svm.SVC(kernel='linear', C=100, probability=True),\n",
    "             KNeighborsClassifier(n_neighbors=6),\n",
    "             RandomForestClassifier(criterion='gini',max_depth=100,n_estimators=10),\n",
    "             GaussianNB()],\n",
    "            \n",
    "            [LogisticRegression(C=1.8, max_iter=1000, multi_class='ovr', solver= 'liblinear'),\n",
    "             svm.SVC(kernel='linear', C=100, probability=True),\n",
    "             KNeighborsClassifier(n_neighbors=6),\n",
    "             RandomForestClassifier(criterion='gini',max_depth=100,n_estimators=10),\n",
    "             GaussianNB()],\n",
    "            \n",
    "            [DecisionTreeClassifier(criterion=\"entropy\", min_samples_split = 200, max_depth=6),\n",
    "             LogisticRegression(C=1.8, max_iter=1000, multi_class='ovr', solver= 'liblinear'),\n",
    "             svm.SVC(kernel='linear', C=100, probability=True),\n",
    "             KNeighborsClassifier(n_neighbors=6)],\n",
    "\n",
    "            [DecisionTreeClassifier(criterion=\"entropy\", min_samples_split = 200, max_depth=6),\n",
    "             LogisticRegression(C=1.8, max_iter=1000, multi_class='ovr', solver= 'liblinear'),\n",
    "             svm.SVC(kernel='linear', C=100, probability=True),\n",
    "             RandomForestClassifier(criterion='gini',max_depth=100,n_estimators=10)],\n",
    " \n",
    "            [DecisionTreeClassifier(criterion=\"entropy\", min_samples_split = 200, max_depth=6),\n",
    "             LogisticRegression(C=1.8, max_iter=1000, multi_class='ovr', solver= 'liblinear'),\n",
    "             svm.SVC(kernel='linear', C=100, probability=True),\n",
    "             GaussianNB()],\n",
    "\n",
    "            [DecisionTreeClassifier(criterion=\"entropy\", min_samples_split = 200, max_depth=6),\n",
    "             LogisticRegression(C=1.8, max_iter=1000, multi_class='ovr', solver= 'liblinear'),\n",
    "             KNeighborsClassifier(n_neighbors=6),\n",
    "             RandomForestClassifier(criterion='gini',max_depth=100,n_estimators=10)],  \n",
    "            \n",
    "            [DecisionTreeClassifier(criterion=\"entropy\", min_samples_split = 200, max_depth=6),\n",
    "             LogisticRegression(C=1.8, max_iter=1000, multi_class='ovr', solver= 'liblinear'),\n",
    "             KNeighborsClassifier(n_neighbors=6), \n",
    "             GaussianNB()],  \n",
    "            \n",
    "            [DecisionTreeClassifier(criterion=\"entropy\", min_samples_split = 200, max_depth=6),\n",
    "             LogisticRegression(C=1.8, max_iter=1000, multi_class='ovr', solver= 'liblinear'),\n",
    "             RandomForestClassifier(criterion='gini',max_depth=100,n_estimators=10), \n",
    "             GaussianNB()],  \n",
    "            \n",
    "            [DecisionTreeClassifier(criterion=\"entropy\", min_samples_split = 200, max_depth=6),\n",
    "             svm.SVC(kernel='linear', C=100, probability=True),\n",
    "             KNeighborsClassifier(n_neighbors=6), \n",
    "             GaussianNB()],  \n",
    " \n",
    "            [DecisionTreeClassifier(criterion=\"entropy\", min_samples_split = 200, max_depth=6),\n",
    "             svm.SVC(kernel='linear', C=100, probability=True),\n",
    "             KNeighborsClassifier(n_neighbors=6),\n",
    "             RandomForestClassifier(criterion='gini',max_depth=100,n_estimators=10)],  \n",
    "            \n",
    "            [DecisionTreeClassifier(criterion=\"entropy\", min_samples_split = 200, max_depth=6),\n",
    "             svm.SVC(kernel='linear', C=100, probability=True),\n",
    "             GaussianNB(),\n",
    "             RandomForestClassifier(criterion='gini',max_depth=100,n_estimators=10)],              \n",
    "\n",
    "            [LogisticRegression(C=1.8, max_iter=1000, multi_class='ovr', solver= 'liblinear'),\n",
    "             svm.SVC(kernel='linear', C=100, probability=True),\n",
    "             KNeighborsClassifier(n_neighbors=6), \n",
    "             GaussianNB()],  \n",
    " \n",
    "            [LogisticRegression(C=1.8, max_iter=1000, multi_class='ovr', solver= 'liblinear'),\n",
    "             svm.SVC(kernel='linear', C=100, probability=True),\n",
    "             KNeighborsClassifier(n_neighbors=6),\n",
    "             RandomForestClassifier(criterion='gini',max_depth=100,n_estimators=10)],  \n",
    "\n",
    "            [LogisticRegression(C=1.8, max_iter=1000, multi_class='ovr', solver= 'liblinear'),\n",
    "             svm.SVC(kernel='linear', C=100, probability=True),\n",
    "             GaussianNB(),\n",
    "             RandomForestClassifier(criterion='gini',max_depth=100,n_estimators=10)],  \n",
    "        \n",
    "            [DecisionTreeClassifier(criterion=\"entropy\", min_samples_split = 200, max_depth=6),\n",
    "             KNeighborsClassifier(n_neighbors=6),\n",
    "             RandomForestClassifier(criterion='gini',max_depth=100,n_estimators=10),\n",
    "             GaussianNB()],  \n",
    "\n",
    "            [LogisticRegression(C=1.8, max_iter=1000, multi_class='ovr', solver= 'liblinear'),\n",
    "             KNeighborsClassifier(n_neighbors=6),\n",
    "             RandomForestClassifier(criterion='gini',max_depth=100,n_estimators=10),\n",
    "             GaussianNB()],  \n",
    "            \n",
    "            [svm.SVC(kernel='linear', C=100, probability=True),\n",
    "             KNeighborsClassifier(n_neighbors=6),\n",
    "             RandomForestClassifier(criterion='gini',max_depth=100,n_estimators=10),\n",
    "             GaussianNB()],  \n",
    "            \n",
    "            [DecisionTreeClassifier(criterion=\"entropy\", min_samples_split = 200, max_depth=6),\n",
    "             LogisticRegression(C=1.8, max_iter=1000, multi_class='ovr', solver= 'liblinear'),\n",
    "             svm.SVC(kernel='linear', C=100, probability=True)],\n",
    "            \n",
    "            [DecisionTreeClassifier(criterion=\"entropy\", min_samples_split = 200, max_depth=6),\n",
    "             LogisticRegression(C=1.8, max_iter=1000, multi_class='ovr', solver= 'liblinear'),\n",
    "             KNeighborsClassifier(n_neighbors=6)],\n",
    "            \n",
    "            [DecisionTreeClassifier(criterion=\"entropy\", min_samples_split = 200, max_depth=6),\n",
    "             svm.SVC(kernel='linear', C=100, probability=True),\n",
    "             KNeighborsClassifier(n_neighbors=6)],\n",
    "            \n",
    "            [LogisticRegression(C=1.8, max_iter=1000, multi_class='ovr', solver= 'liblinear'),\n",
    "             svm.SVC(kernel='linear', C=100, probability=True),\n",
    "             KNeighborsClassifier(n_neighbors=6)],\n",
    "            \n",
    "            [DecisionTreeClassifier(criterion=\"entropy\", min_samples_split = 200, max_depth=6),\n",
    "             LogisticRegression(C=1.8, max_iter=1000, multi_class='ovr', solver= 'liblinear'),\n",
    "             RandomForestClassifier(criterion='gini',max_depth=100,n_estimators=10)],\n",
    "            \n",
    "            [DecisionTreeClassifier(criterion=\"entropy\", min_samples_split = 200, max_depth=6),\n",
    "             svm.SVC(kernel='linear', C=100, probability=True),\n",
    "             RandomForestClassifier(criterion='gini',max_depth=100,n_estimators=10)],\n",
    "            \n",
    "            [LogisticRegression(C=1.8, max_iter=1000, multi_class='ovr', solver= 'liblinear'),\n",
    "             svm.SVC(kernel='linear', C=100, probability=True),\n",
    "             RandomForestClassifier(criterion='gini',max_depth=100,n_estimators=10)],\n",
    "                     \n",
    "            [DecisionTreeClassifier(criterion=\"entropy\", min_samples_split = 200, max_depth=6),\n",
    "             LogisticRegression(C=1.8, max_iter=1000, multi_class='ovr', solver= 'liblinear'),\n",
    "             GaussianNB()],\n",
    "            \n",
    "            [DecisionTreeClassifier(criterion=\"entropy\", min_samples_split = 200, max_depth=6),\n",
    "             svm.SVC(kernel='linear', C=100, probability=True),\n",
    "             GaussianNB()],\n",
    "            \n",
    "            [LogisticRegression(C=1.8, max_iter=1000, multi_class='ovr', solver= 'liblinear'),\n",
    "             svm.SVC(kernel='linear', C=100, probability=True),\n",
    "             GaussianNB()],\n",
    "\n",
    "            [DecisionTreeClassifier(criterion=\"entropy\", min_samples_split = 200, max_depth=6),\n",
    "             KNeighborsClassifier(n_neighbors=6),\n",
    "             RandomForestClassifier(criterion='gini',max_depth=100,n_estimators=10)],\n",
    "                        \n",
    "            [DecisionTreeClassifier(criterion=\"entropy\", min_samples_split = 200, max_depth=6),\n",
    "             KNeighborsClassifier(n_neighbors=6),\n",
    "             GaussianNB()],\n",
    "                             \n",
    "            [DecisionTreeClassifier(criterion=\"entropy\", min_samples_split = 200, max_depth=6),\n",
    "             RandomForestClassifier(criterion='gini',max_depth=100,n_estimators=10),\n",
    "             GaussianNB()],\n",
    "            \n",
    "            [LogisticRegression(C=1.8, max_iter=1000, multi_class='ovr', solver= 'liblinear'),\n",
    "             KNeighborsClassifier(n_neighbors=6),\n",
    "             RandomForestClassifier(criterion='gini',max_depth=100,n_estimators=10)],\n",
    "                        \n",
    "            [LogisticRegression(C=1.8, max_iter=1000, multi_class='ovr', solver= 'liblinear'),\n",
    "             KNeighborsClassifier(n_neighbors=6),\n",
    "             GaussianNB()],\n",
    "                             \n",
    "            [LogisticRegression(C=1.8, max_iter=1000, multi_class='ovr', solver= 'liblinear'),\n",
    "             RandomForestClassifier(criterion='gini',max_depth=100,n_estimators=10),\n",
    "             GaussianNB()],\n",
    "\n",
    "            [svm.SVC(kernel='linear', C=100, probability=True),\n",
    "             KNeighborsClassifier(n_neighbors=6),\n",
    "             RandomForestClassifier(criterion='gini',max_depth=100,n_estimators=10)],\n",
    "                        \n",
    "            [svm.SVC(kernel='linear', C=100, probability=True),\n",
    "             KNeighborsClassifier(n_neighbors=6),\n",
    "             GaussianNB()],\n",
    "                             \n",
    "            [svm.SVC(kernel='linear', C=100, probability=True),\n",
    "             RandomForestClassifier(criterion='gini',max_depth=100,n_estimators=10),\n",
    "             GaussianNB()],\n",
    "            \n",
    "            [KNeighborsClassifier(n_neighbors=6),\n",
    "             RandomForestClassifier(criterion='gini',max_depth=100,n_estimators=10),\n",
    "             GaussianNB()],\n",
    "                               \n",
    "            [LogisticRegression(C=1.8, max_iter=1000, multi_class='ovr', solver= 'liblinear'),\n",
    "             svm.SVC(kernel='linear', C=100, probability=True)],                     \n",
    "                            \n",
    "            [LogisticRegression(C=1.8, max_iter=1000, multi_class='ovr', solver= 'liblinear'),\n",
    "             DecisionTreeClassifier(criterion=\"entropy\", min_samples_split = 200, max_depth=6)],                      \n",
    "\n",
    "            [DecisionTreeClassifier(criterion=\"entropy\", min_samples_split = 200, max_depth=6),\n",
    "             svm.SVC(kernel='linear', C=100, probability=True)]                      \n",
    "        \n",
    "        ]\n",
    "        \n",
    "        for i in range(45):\n",
    "            if self.base_clsf_type==i:\n",
    "                self.clfs_ = base_clfs_comb[i]\n",
    "  \n",
    "        #To avoid overfitting the generation of the stacked layer training set uses a k-fold cross validation process\n",
    "        #check_cv - Input checker utility for building a cross-validator\n",
    "        #The return value is a cross-validator which generates the train/test splits via the split method.\n",
    "        cross_validator = check_cv(self.cv, y, classifier=self.stratify)\n",
    "        if isinstance(self.cv, int):\n",
    "            # In case of self generated cross-validation strategy override shuffle parameter\n",
    "            cross_validator.shuffle = self.shuffle\n",
    "        skf = list(cross_validator.split(X, y))\n",
    "        \n",
    "        #Training the base classifiers and generation of the stacked layer training set\n",
    "        clfs_outputs = np.array([]).reshape(len(y), 0) #base classifiers predictions array\n",
    "        \n",
    "        for clf in self.clfs_:\n",
    "            if not self.probability_outputs:\n",
    "                clf_outputs = np.array([]).reshape(0, 1) #single base classifier predictions array\n",
    "            else:\n",
    "                clf_outputs = np.array([]).reshape(0, len(set(y))) #every classifier will have 10 columns - probability predictions for each class labels\n",
    "                \n",
    "            for counter, (training_index, testing_index) in enumerate(skf):\n",
    "                #To further add variety to the base estimators a bootstrap sampling (bagging) is used each time we train one one of the base estimators\n",
    "                my_model = ensemble.BaggingClassifier(base_estimator = clf, bootstrap=True, max_samples= 1.0)\n",
    "                my_model.fit(X[training_index], y[training_index])  # training / fitting\n",
    "                \n",
    "                if not self.probability_outputs:                   #testing /predicting\n",
    "                    predictions = my_model.predict(X[testing_index])\n",
    "                    predictions = predictions.reshape(predictions.shape[0], 1)\n",
    "                else:\n",
    "                    predictions = my_model.predict_proba(X[testing_index])\n",
    "                clf_outputs = np.vstack([clf_outputs.astype(predictions.dtype),predictions])\n",
    "\n",
    "            clfs_outputs = np.hstack([clfs_outputs.astype(clf_outputs.dtype),clf_outputs]) #stacked layer training set                \n",
    "        \n",
    "    \n",
    "        #Rearranging the Target/Labels and original input 'X' so they are in line with the way we generated predictions \n",
    "        #via stratified k-fold cross-validadion (because of shaffeling)\n",
    "        rearranged_labels = np.array([]).astype(y.dtype)\n",
    "        rearranged_X = np.array([]).reshape((0, X.shape[1])).astype(X.dtype)\n",
    "        for training_index, testing_index in skf:\n",
    "            rearranged_labels = np.concatenate((rearranged_labels,y[testing_index]))\n",
    "            rearranged_X = np.concatenate((rearranged_X, X[testing_index]))\n",
    "\n",
    "        # Training the base models on the entire training set for the 'base_classifiers_predictions' method\n",
    "        for model in self.clfs_:\n",
    "            model.fit(X, y)\n",
    "            \n",
    "\n",
    "        #Training / fitting the stacked layer classifier\n",
    "        if self.add_original_input: #Task 8 \n",
    "            self.s_clf_.fit(np.hstack((rearranged_X, clfs_outputs)),rearranged_labels) # adding the original input 'X' (but with correctly oredered features)  \n",
    "                                                            #to the input 'clfs_outputs' at the stack layer \n",
    "        else: \n",
    "            self.s_clf_.fit(clfs_outputs,rearranged_labels) #Training the stacked layer classifier on the base classifiers outputs clfs_outputs\n",
    "                                            #that are combined along with the training dataset labels 'rearranged_labels'\n",
    "            \n",
    "        # Return the classifier\n",
    "        return self\n",
    "\n",
    "    def base_classifiers_predictions(self, X):\n",
    "        #The generation of the stacked layer training set\n",
    "        #'column_stack' takes a sequence of 1-D arrays - base classifiers \n",
    "        #predictions and stack them as columns\n",
    "        if self.probability_outputs: #Task 3 - training on the probability outputs from the base classifiers\n",
    "            probability_array = np.asarray([clf.predict_proba(X) for clf in self.clfs_])\n",
    "            clfs_outputs = np.concatenate(probability_array, axis=1) \n",
    "            #without this method I am getting ValueError: X has 3 features per sample; expecting 9\n",
    "            #that could be solved with 'transform' but this is more elegant\n",
    "        else: #training on the label outputs from the base classifiers\n",
    "            clfs_outputs = np.column_stack([clf.predict(X) for clf in self.clfs_])\n",
    "        return clfs_outputs\n",
    "    \n",
    "    # The predict function to make a set of predictions for a \n",
    "    #set of query instances\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels of the input samples X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like matrix of shape = [n_samples, n_features]\n",
    "            The input samples. \n",
    "        Returns\n",
    "        -------\n",
    "        p : array of shape = [n_samples, ].\n",
    "            The predicted class labels of the input samples. \n",
    "        \"\"\"\n",
    "        #Stacking base classifiers predictions as the columns for the training set for the stack layer classifier\n",
    "        clfs_outputs = self.base_classifiers_predictions(X)\n",
    "        \n",
    "        #Making a set of predictions for the training data\n",
    "        if self.add_original_input: \n",
    "            return self.s_clf_.predict(np.hstack((X, clfs_outputs))) # adding the original input 'X' to the input 'clfs_outputs'                                                     #at the stack layer \n",
    "        else:\n",
    "            return self.s_clf_.predict(clfs_outputs) #Making a set of predictions on the base classifiers outputs\n",
    "    \n",
    "    # The predict function to make a set of predictions for a \n",
    "    #set of query instances\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict class probabilities of the input samples X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like matrix of shape = [n_samples, n_features]\n",
    "            The input samples. \n",
    "        Returns\n",
    "        -------\n",
    "        p : array of shape = [n_samples, n_labels].\n",
    "            The predicted class label probabilities of the input samples. \n",
    "        \"\"\"\n",
    "        clfs_outputs = self.base_classifiers_predictions(X)\n",
    "        \n",
    "        if self.add_original_input: \n",
    "            return self.s_clf_.predict_proba(np.hstack((X, clfs_outputs))) # adding the original input 'X' to the input 'clfs_outputs' \n",
    "                                                            #at the stack layer \n",
    "        else:\n",
    "            return self.s_clf_.predict_proba(clfs_outputs) #Making a set of predictions on the base classifiers outputs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the SuperLearnerClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a simple test using the SuperLearnClassifier on the Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.        ,  1.        ,  1.        ,  1.        ,  0.86666667,\n",
       "        0.93333333,  0.86666667,  1.        ,  1.        ,  1.        ])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "clf = SuperLearnerClassifier(type_of_model=1, base_clsf_type=35)\n",
    "iris = load_iris()\n",
    "clf.fit(iris.data, iris.target) #Training data has shape (150, 9) for probability_outputs=True\n",
    "cross_val_score(clf, iris.data, iris.target, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree 10-fold cross validation scores:\n",
      " [ 0.33333333  0.33333333  0.33333333  0.33333333  0.33333333  0.33333333\n",
      "  0.33333333  0.33333333  0.33333333  0.33333333]\n",
      "Accuracy (average score):  0.33 ---> Decision Tree \n",
      "\n",
      "Logistic Regression 10-fold cross validation scores:\n",
      " [ 1.          1.          1.          0.93333333  0.93333333  0.93333333\n",
      "  0.8         1.          1.          1.        ]\n",
      "Accuracy (average score):  0.96 ---> Logistic Regression \n",
      "\n",
      "Support Vector Machine 10-fold cross validation scores:\n",
      " [ 1.          1.          1.          1.          0.93333333  1.\n",
      "  0.86666667  0.93333333  1.          1.        ]\n",
      "Accuracy (average score):  0.97 ---> Support Vector Machine \n",
      "\n",
      "KNN 10-fold cross validation scores:\n",
      " [ 1.          0.93333333  1.          1.          0.86666667  0.93333333\n",
      "  0.93333333  1.          1.          1.        ]\n",
      "Accuracy (average score):  0.97 ---> KNN \n",
      "\n",
      "Random Forest 10-fold cross validation scores:\n",
      " [ 1.          0.93333333  1.          0.93333333  0.86666667  0.86666667\n",
      "  0.93333333  0.93333333  1.          1.        ]\n",
      "Accuracy (average score):  0.95 ---> Random Forest \n",
      "\n",
      "Naive Bayes 10-fold cross validation scores:\n",
      " [ 0.93333333  0.93333333  1.          0.93333333  0.93333333  0.93333333\n",
      "  0.86666667  1.          1.          1.        ]\n",
      "Accuracy (average score):  0.95 ---> Naive Bayes \n",
      "\n"
     ]
    }
   ],
   "source": [
    "data, target = iris.data, iris.target\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "clf0 = DecisionTreeClassifier(criterion=\"entropy\", min_samples_split = 200, max_depth=6)\n",
    "clf1 = LogisticRegression(C=1.8, max_iter=1000, multi_class='ovr', solver= 'liblinear')\n",
    "clf2 = svm.SVC(kernel='linear', C=100, probability=True)\n",
    "clf3 = KNeighborsClassifier(n_neighbors=6)\n",
    "clf4 = RandomForestClassifier(criterion='gini',max_depth=100,n_estimators=10)\n",
    "clf5 = GaussianNB()                             \n",
    "                              \n",
    "stacked_clf = SuperLearnerClassifier(type_of_model=1, base_clsf_type=35)\n",
    "\n",
    "\n",
    "#checking the accurancy of the Super Learner and each base classifier individualy \n",
    "for model, name in zip([clf0, clf1, clf2, clf3, clf4, clf5, stacked_clf], \n",
    "                      ['Decision Tree', \n",
    "                       'Logistic Regression', \n",
    "                       'Support Vector Machine',\n",
    "                       'KNN',\n",
    "                       'Random Forest',\n",
    "                       'Naive Bayes', \n",
    "                       'SuperLearnerClassifier',]):\n",
    "\n",
    "    scores = model_selection.cross_val_score(model, data, target, cv=10, scoring='accuracy')\n",
    "    print(name, '10-fold cross validation scores:\\n',scores)\n",
    "    print(\"Accuracy (average score):  %0.2f ---> %s\" \n",
    "          % (scores.mean(), name),'\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load & Partition Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup - IMPORTANT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take only a sample of the dataset for fast testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_sampling_rate = 0.05 #Setting up a data sampling rate for speeding up testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the number of folds for all grid searches (should be 5 - 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv_folds = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up a dictionary to store simple model perofrmance comparions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_test_accuracy_comparisons = dict()\n",
    "model_valid_accuracy_comparisons = dict()\n",
    "model_tuned_params_list = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Train Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset and explore it. --> Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19987</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>119</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58105</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>189</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1732</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25670</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19234</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "19987      1       0       0       0       0       0       0       0       0   \n",
       "58105      3       0       0       0       0       0       0       0       0   \n",
       "1732       2       0       0       0       0       0       0       0       0   \n",
       "25670      1       0       0       0       0       0       0       0       0   \n",
       "19234      8       0       0       0       0       0       0       0       0   \n",
       "\n",
       "       pixel9    ...     pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "19987       0    ...          119         0         0         0         0   \n",
       "58105       0    ...          189         0         0         3         0   \n",
       "1732        0    ...            0         0         0         1        62   \n",
       "25670       0    ...            0         0         0         0         0   \n",
       "19234       0    ...            0         0         0         0         0   \n",
       "\n",
       "       pixel780  pixel781  pixel782  pixel783  pixel784  \n",
       "19987         0         0         0         0         0  \n",
       "58105         0         0         0         0         0  \n",
       "1732         58         0         0         0         0  \n",
       "25670         0         0         0         0         0  \n",
       "19234         0         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = pd.read_csv('fashion-mnist_train.csv')\n",
    "dataset = dataset.sample(frac=data_sampling_rate) #take a sample from the dataset \n",
    "                                                    #so everyhting runs smoothly\n",
    "num_classes = 10\n",
    "classes = {0: \"T-shirt/top\", 1:\"Trouser\", 2: \"Pullover\", 3:\"Dress\", 4:\"Coat\", \n",
    "           5:\"Sandal\", 6:\"Shirt\", 7:\"Sneaker\", 8:\"Bag\", 9:\"Ankle boot\"}\n",
    "display(dataset.head()) #dataset.shape = (3000, 785)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining the distribution of the ten classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    326\n",
       "1    321\n",
       "9    309\n",
       "8    303\n",
       "7    293\n",
       "3    293\n",
       "4    292\n",
       "2    290\n",
       "0    287\n",
       "6    286\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"label\"].value_counts()\n",
    "#dataset['label']=dataset['label'].astype(object)\n",
    "#dataset.select_dtypes(include=[np.object]).shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.516667</td>\n",
       "      <td>0.000667</td>\n",
       "      <td>0.006333</td>\n",
       "      <td>0.056333</td>\n",
       "      <td>0.137667</td>\n",
       "      <td>0.268000</td>\n",
       "      <td>0.409333</td>\n",
       "      <td>0.848000</td>\n",
       "      <td>1.823000</td>\n",
       "      <td>4.922333</td>\n",
       "      <td>...</td>\n",
       "      <td>35.663000</td>\n",
       "      <td>23.339667</td>\n",
       "      <td>15.564000</td>\n",
       "      <td>16.527000</td>\n",
       "      <td>21.885667</td>\n",
       "      <td>16.943667</td>\n",
       "      <td>8.250667</td>\n",
       "      <td>2.485000</td>\n",
       "      <td>0.907333</td>\n",
       "      <td>0.036667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.876424</td>\n",
       "      <td>0.036515</td>\n",
       "      <td>0.260094</td>\n",
       "      <td>1.960895</td>\n",
       "      <td>3.306714</td>\n",
       "      <td>4.806094</td>\n",
       "      <td>5.610431</td>\n",
       "      <td>8.706942</td>\n",
       "      <td>12.115002</td>\n",
       "      <td>21.077549</td>\n",
       "      <td>...</td>\n",
       "      <td>58.472607</td>\n",
       "      <td>49.033176</td>\n",
       "      <td>40.605511</td>\n",
       "      <td>42.683868</td>\n",
       "      <td>50.887547</td>\n",
       "      <td>44.413196</td>\n",
       "      <td>29.340838</td>\n",
       "      <td>16.688178</td>\n",
       "      <td>9.731237</td>\n",
       "      <td>0.936099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>106.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>166.000000</td>\n",
       "      <td>161.000000</td>\n",
       "      <td>188.000000</td>\n",
       "      <td>219.000000</td>\n",
       "      <td>219.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>244.000000</td>\n",
       "      <td>252.000000</td>\n",
       "      <td>243.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>244.000000</td>\n",
       "      <td>237.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>255.000000</td>\n",
       "      <td>38.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             label       pixel1       pixel2       pixel3       pixel4  \\\n",
       "count  3000.000000  3000.000000  3000.000000  3000.000000  3000.000000   \n",
       "mean      4.516667     0.000667     0.006333     0.056333     0.137667   \n",
       "std       2.876424     0.036515     0.260094     1.960895     3.306714   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       2.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       5.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       7.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "max       9.000000     2.000000    14.000000   106.000000   150.000000   \n",
       "\n",
       "            pixel5       pixel6       pixel7       pixel8       pixel9  \\\n",
       "count  3000.000000  3000.000000  3000.000000  3000.000000  3000.000000   \n",
       "mean      0.268000     0.409333     0.848000     1.823000     4.922333   \n",
       "std       4.806094     5.610431     8.706942    12.115002    21.077549   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "max     166.000000   161.000000   188.000000   219.000000   219.000000   \n",
       "\n",
       "          ...          pixel775     pixel776     pixel777     pixel778  \\\n",
       "count     ...       3000.000000  3000.000000  3000.000000  3000.000000   \n",
       "mean      ...         35.663000    23.339667    15.564000    16.527000   \n",
       "std       ...         58.472607    49.033176    40.605511    42.683868   \n",
       "min       ...          0.000000     0.000000     0.000000     0.000000   \n",
       "25%       ...          0.000000     0.000000     0.000000     0.000000   \n",
       "50%       ...          0.000000     0.000000     0.000000     0.000000   \n",
       "75%       ...         58.000000     7.000000     0.000000     0.000000   \n",
       "max       ...        244.000000   252.000000   243.000000   255.000000   \n",
       "\n",
       "          pixel779     pixel780     pixel781     pixel782     pixel783  \\\n",
       "count  3000.000000  3000.000000  3000.000000  3000.000000  3000.000000   \n",
       "mean     21.885667    16.943667     8.250667     2.485000     0.907333   \n",
       "std      50.887547    44.413196    29.340838    16.688178     9.731237   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "max     250.000000   244.000000   237.000000   255.000000   255.000000   \n",
       "\n",
       "          pixel784  \n",
       "count  3000.000000  \n",
       "mean      0.036667  \n",
       "std       0.936099  \n",
       "min       0.000000  \n",
       "25%       0.000000  \n",
       "50%       0.000000  \n",
       "75%       0.000000  \n",
       "max      38.000000  \n",
       "\n",
       "[8 rows x 785 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Describing columns that are numeric dtypes; it will include: \n",
    "#count, mean, std, min, max, and lower, 50, and upper percentiles.\n",
    "if(dataset.select_dtypes(include=[np.number]).shape[1] > 0):\n",
    "    display(dataset.select_dtypes(include=[np.number]).describe()) \n",
    "#Describing columns that are object dtypes; For object dtypes (e.g. timestamps or strings), \n",
    "#the index will include the count, unique, most common, and frequency of the most common. \n",
    "if(dataset.select_dtypes(include=[np.object]).shape[1] > 0):\n",
    "    display(dataset.select_dtypes(include=[np.object]).describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Check for presence of missing values\n",
    "print(\"Missing Values\")\n",
    "#print(dataset.isnull().sum())\n",
    "print(sum(dataset.isnull().sum())) #no missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process & Partition Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform data pre-processing and manipulation as required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Isolating the descriptive features we are interested in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = dataset[dataset.columns[1:]] #X.shape=(3000, 784)\n",
    "Y = np.array(dataset[\"label\"]) #Y.shape=(3000,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardization of Data - transforming data columns to have zero mean and unit variance\n",
    "Standardized_X = (X â€“ Average) / Std_Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = (X - X.values.mean()) / X.values.std()\n",
    "#print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalise the data (important for some models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = (X - X.values.min())/ (X.values.max() - X.values.min())  #Normalized_X = (X â€“ min)/(max-min)  max= 255, min = 0 --> #X=X/255 \n",
    "#print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#X=X/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract some higher level features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>percent_filled</th>\n",
       "      <th>percent_filled_top</th>\n",
       "      <th>percent_filled_bottom</th>\n",
       "      <th>row_sum_0</th>\n",
       "      <th>row_sum_1</th>\n",
       "      <th>row_sum_2</th>\n",
       "      <th>row_sum_3</th>\n",
       "      <th>row_sum_4</th>\n",
       "      <th>row_sum_5</th>\n",
       "      <th>row_sum_6</th>\n",
       "      <th>...</th>\n",
       "      <th>row_sum_19</th>\n",
       "      <th>row_sum_20</th>\n",
       "      <th>row_sum_21</th>\n",
       "      <th>row_sum_22</th>\n",
       "      <th>row_sum_23</th>\n",
       "      <th>row_sum_24</th>\n",
       "      <th>row_sum_25</th>\n",
       "      <th>row_sum_26</th>\n",
       "      <th>row_sum_27</th>\n",
       "      <th>symmetry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19987</th>\n",
       "      <td>0.235909</td>\n",
       "      <td>0.283303</td>\n",
       "      <td>0.188515</td>\n",
       "      <td>0.089776</td>\n",
       "      <td>0.315546</td>\n",
       "      <td>0.290896</td>\n",
       "      <td>0.301541</td>\n",
       "      <td>0.315406</td>\n",
       "      <td>0.325490</td>\n",
       "      <td>0.337255</td>\n",
       "      <td>...</td>\n",
       "      <td>0.202101</td>\n",
       "      <td>0.203501</td>\n",
       "      <td>0.199720</td>\n",
       "      <td>0.185714</td>\n",
       "      <td>0.180812</td>\n",
       "      <td>0.175070</td>\n",
       "      <td>0.161064</td>\n",
       "      <td>0.161625</td>\n",
       "      <td>0.077731</td>\n",
       "      <td>0.080357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58105</th>\n",
       "      <td>0.318102</td>\n",
       "      <td>0.327661</td>\n",
       "      <td>0.308543</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.139076</td>\n",
       "      <td>0.404622</td>\n",
       "      <td>0.431092</td>\n",
       "      <td>0.423810</td>\n",
       "      <td>0.433613</td>\n",
       "      <td>0.430392</td>\n",
       "      <td>...</td>\n",
       "      <td>0.324510</td>\n",
       "      <td>0.323529</td>\n",
       "      <td>0.322969</td>\n",
       "      <td>0.318908</td>\n",
       "      <td>0.315966</td>\n",
       "      <td>0.310924</td>\n",
       "      <td>0.305042</td>\n",
       "      <td>0.327451</td>\n",
       "      <td>0.243557</td>\n",
       "      <td>0.153061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1732</th>\n",
       "      <td>0.225570</td>\n",
       "      <td>0.186014</td>\n",
       "      <td>0.265126</td>\n",
       "      <td>0.037815</td>\n",
       "      <td>0.076751</td>\n",
       "      <td>0.097199</td>\n",
       "      <td>0.134034</td>\n",
       "      <td>0.162605</td>\n",
       "      <td>0.155462</td>\n",
       "      <td>0.170448</td>\n",
       "      <td>...</td>\n",
       "      <td>0.295658</td>\n",
       "      <td>0.299160</td>\n",
       "      <td>0.307843</td>\n",
       "      <td>0.323109</td>\n",
       "      <td>0.332493</td>\n",
       "      <td>0.335714</td>\n",
       "      <td>0.285994</td>\n",
       "      <td>0.119188</td>\n",
       "      <td>0.042297</td>\n",
       "      <td>0.003827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25670</th>\n",
       "      <td>0.140006</td>\n",
       "      <td>0.156753</td>\n",
       "      <td>0.123259</td>\n",
       "      <td>0.131092</td>\n",
       "      <td>0.158683</td>\n",
       "      <td>0.130812</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.146218</td>\n",
       "      <td>0.151261</td>\n",
       "      <td>0.159944</td>\n",
       "      <td>...</td>\n",
       "      <td>0.121148</td>\n",
       "      <td>0.116246</td>\n",
       "      <td>0.114286</td>\n",
       "      <td>0.110084</td>\n",
       "      <td>0.109524</td>\n",
       "      <td>0.111204</td>\n",
       "      <td>0.109944</td>\n",
       "      <td>0.113445</td>\n",
       "      <td>0.092997</td>\n",
       "      <td>0.014031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19234</th>\n",
       "      <td>0.506878</td>\n",
       "      <td>0.498850</td>\n",
       "      <td>0.514906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.358543</td>\n",
       "      <td>0.557003</td>\n",
       "      <td>0.676190</td>\n",
       "      <td>...</td>\n",
       "      <td>0.793277</td>\n",
       "      <td>0.780672</td>\n",
       "      <td>0.769328</td>\n",
       "      <td>0.626331</td>\n",
       "      <td>0.199160</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.276786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30565</th>\n",
       "      <td>0.168202</td>\n",
       "      <td>0.182143</td>\n",
       "      <td>0.154262</td>\n",
       "      <td>0.164566</td>\n",
       "      <td>0.218207</td>\n",
       "      <td>0.156583</td>\n",
       "      <td>0.168207</td>\n",
       "      <td>0.170028</td>\n",
       "      <td>0.190616</td>\n",
       "      <td>0.187675</td>\n",
       "      <td>...</td>\n",
       "      <td>0.148039</td>\n",
       "      <td>0.148179</td>\n",
       "      <td>0.149580</td>\n",
       "      <td>0.155602</td>\n",
       "      <td>0.156162</td>\n",
       "      <td>0.149440</td>\n",
       "      <td>0.149020</td>\n",
       "      <td>0.152661</td>\n",
       "      <td>0.124090</td>\n",
       "      <td>0.058673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8409</th>\n",
       "      <td>0.359544</td>\n",
       "      <td>0.342787</td>\n",
       "      <td>0.376301</td>\n",
       "      <td>0.133754</td>\n",
       "      <td>0.157843</td>\n",
       "      <td>0.162325</td>\n",
       "      <td>0.278711</td>\n",
       "      <td>0.335154</td>\n",
       "      <td>0.368908</td>\n",
       "      <td>0.384034</td>\n",
       "      <td>...</td>\n",
       "      <td>0.496779</td>\n",
       "      <td>0.492157</td>\n",
       "      <td>0.519608</td>\n",
       "      <td>0.450560</td>\n",
       "      <td>0.186275</td>\n",
       "      <td>0.183473</td>\n",
       "      <td>0.182773</td>\n",
       "      <td>0.182493</td>\n",
       "      <td>0.152801</td>\n",
       "      <td>0.174745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40807</th>\n",
       "      <td>0.169488</td>\n",
       "      <td>0.159754</td>\n",
       "      <td>0.179222</td>\n",
       "      <td>0.041737</td>\n",
       "      <td>0.107283</td>\n",
       "      <td>0.174370</td>\n",
       "      <td>0.205602</td>\n",
       "      <td>0.173109</td>\n",
       "      <td>0.146359</td>\n",
       "      <td>0.161204</td>\n",
       "      <td>...</td>\n",
       "      <td>0.175490</td>\n",
       "      <td>0.176891</td>\n",
       "      <td>0.186975</td>\n",
       "      <td>0.188655</td>\n",
       "      <td>0.189076</td>\n",
       "      <td>0.194398</td>\n",
       "      <td>0.185854</td>\n",
       "      <td>0.209244</td>\n",
       "      <td>0.141036</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28745</th>\n",
       "      <td>0.193397</td>\n",
       "      <td>0.129652</td>\n",
       "      <td>0.257143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023109</td>\n",
       "      <td>...</td>\n",
       "      <td>0.597759</td>\n",
       "      <td>0.559384</td>\n",
       "      <td>0.193137</td>\n",
       "      <td>0.000560</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43967</th>\n",
       "      <td>0.262680</td>\n",
       "      <td>0.259614</td>\n",
       "      <td>0.265746</td>\n",
       "      <td>0.187395</td>\n",
       "      <td>0.289076</td>\n",
       "      <td>0.250140</td>\n",
       "      <td>0.271148</td>\n",
       "      <td>0.268067</td>\n",
       "      <td>0.304902</td>\n",
       "      <td>0.325350</td>\n",
       "      <td>...</td>\n",
       "      <td>0.260644</td>\n",
       "      <td>0.283473</td>\n",
       "      <td>0.282353</td>\n",
       "      <td>0.298319</td>\n",
       "      <td>0.312045</td>\n",
       "      <td>0.314706</td>\n",
       "      <td>0.301401</td>\n",
       "      <td>0.304342</td>\n",
       "      <td>0.190616</td>\n",
       "      <td>0.012755</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       percent_filled  percent_filled_top  percent_filled_bottom  row_sum_0  \\\n",
       "19987        0.235909            0.283303               0.188515   0.089776   \n",
       "58105        0.318102            0.327661               0.308543   0.000000   \n",
       "1732         0.225570            0.186014               0.265126   0.037815   \n",
       "25670        0.140006            0.156753               0.123259   0.131092   \n",
       "19234        0.506878            0.498850               0.514906   0.000000   \n",
       "30565        0.168202            0.182143               0.154262   0.164566   \n",
       "8409         0.359544            0.342787               0.376301   0.133754   \n",
       "40807        0.169488            0.159754               0.179222   0.041737   \n",
       "28745        0.193397            0.129652               0.257143   0.000000   \n",
       "43967        0.262680            0.259614               0.265746   0.187395   \n",
       "\n",
       "       row_sum_1  row_sum_2  row_sum_3  row_sum_4  row_sum_5  row_sum_6  \\\n",
       "19987   0.315546   0.290896   0.301541   0.315406   0.325490   0.337255   \n",
       "58105   0.139076   0.404622   0.431092   0.423810   0.433613   0.430392   \n",
       "1732    0.076751   0.097199   0.134034   0.162605   0.155462   0.170448   \n",
       "25670   0.158683   0.130812   0.142857   0.146218   0.151261   0.159944   \n",
       "19234   0.000000   0.000000   0.000000   0.358543   0.557003   0.676190   \n",
       "30565   0.218207   0.156583   0.168207   0.170028   0.190616   0.187675   \n",
       "8409    0.157843   0.162325   0.278711   0.335154   0.368908   0.384034   \n",
       "40807   0.107283   0.174370   0.205602   0.173109   0.146359   0.161204   \n",
       "28745   0.000000   0.000000   0.000000   0.000000   0.000000   0.023109   \n",
       "43967   0.289076   0.250140   0.271148   0.268067   0.304902   0.325350   \n",
       "\n",
       "         ...     row_sum_19  row_sum_20  row_sum_21  row_sum_22  row_sum_23  \\\n",
       "19987    ...       0.202101    0.203501    0.199720    0.185714    0.180812   \n",
       "58105    ...       0.324510    0.323529    0.322969    0.318908    0.315966   \n",
       "1732     ...       0.295658    0.299160    0.307843    0.323109    0.332493   \n",
       "25670    ...       0.121148    0.116246    0.114286    0.110084    0.109524   \n",
       "19234    ...       0.793277    0.780672    0.769328    0.626331    0.199160   \n",
       "30565    ...       0.148039    0.148179    0.149580    0.155602    0.156162   \n",
       "8409     ...       0.496779    0.492157    0.519608    0.450560    0.186275   \n",
       "40807    ...       0.175490    0.176891    0.186975    0.188655    0.189076   \n",
       "28745    ...       0.597759    0.559384    0.193137    0.000560    0.000000   \n",
       "43967    ...       0.260644    0.283473    0.282353    0.298319    0.312045   \n",
       "\n",
       "       row_sum_24  row_sum_25  row_sum_26  row_sum_27  symmetry  \n",
       "19987    0.175070    0.161064    0.161625    0.077731  0.080357  \n",
       "58105    0.310924    0.305042    0.327451    0.243557  0.153061  \n",
       "1732     0.335714    0.285994    0.119188    0.042297  0.003827  \n",
       "25670    0.111204    0.109944    0.113445    0.092997  0.014031  \n",
       "19234    0.000000    0.000000    0.000000    0.000000  0.276786  \n",
       "30565    0.149440    0.149020    0.152661    0.124090  0.058673  \n",
       "8409     0.183473    0.182773    0.182493    0.152801  0.174745  \n",
       "40807    0.194398    0.185854    0.209244    0.141036  0.000000  \n",
       "28745    0.000000    0.000000    0.000000    0.000000  0.047194  \n",
       "43967    0.314706    0.301401    0.304342    0.190616  0.012755  \n",
       "\n",
       "[10 rows x 32 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "engineered_features = pd.DataFrame()\n",
    "\n",
    "# Calcualte percentage of filled pixels and a top and bottom half only version\n",
    "percent_filled = X.sum(axis = 1)/(28*28)\n",
    "percent_filled_top = X.iloc[:, 0:392].sum(axis = 1)/(28*14)\n",
    "percent_filled_bottom = X.iloc[:, 392:784].sum(axis = 1)/(28*14)\n",
    "engineered_features['percent_filled'] = percent_filled\n",
    "engineered_features['percent_filled_top'] = percent_filled_top\n",
    "engineered_features['percent_filled_bottom'] = percent_filled_bottom\n",
    "\n",
    "#print(engineered_features)\n",
    "# Calculate the sum of each row\n",
    "for idx, i in enumerate(range(0, 784, 28)):\n",
    "    row_sum = X.iloc[:, i:(i + 28)].sum(axis = 1)/28\n",
    "    engineered_features[\"row_sum_\" + str(idx)] = row_sum\n",
    "\n",
    "# Calcualte a measure of syymmetry around a horizontal axis\n",
    "s1 = np.round(np.array(X.iloc[:, 0:392]))\n",
    "s2 = np.round(np.array(X.iloc[:, 784:391:-1]))\n",
    "s3 = np.logical_and(s1, s2).astype(int)\n",
    "symmetry = s3.sum(axis = 1)/(28*28)\n",
    "\n",
    "engineered_features['symmetry'] = symmetry\n",
    "display(engineered_features.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 32)\n"
     ]
    }
   ],
   "source": [
    "X = engineered_features\n",
    "print(X.shape) #With engineered features we reduced dataset from 784 columns to 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spliting the data into a **training set**, a **vaidation set**, and a **test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aidasehic/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X_train_plus_valid, X_test, y_train_plus_valid, y_test \\\n",
    "    = train_test_split(X, Y, random_state=0, train_size = 0.7)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid \\\n",
    "    = train_test_split(X_train_plus_valid, \\\n",
    "                                        y_train_plus_valid, \\\n",
    "                                        random_state=0, \\\n",
    "                                        train_size = 0.5/0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Test Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imprting Fashion MNIST Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>103</td>\n",
       "      <td>87</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>53</td>\n",
       "      <td>99</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>53</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>137</td>\n",
       "      <td>126</td>\n",
       "      <td>140</td>\n",
       "      <td>0</td>\n",
       "      <td>133</td>\n",
       "      <td>224</td>\n",
       "      <td>222</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "0      0       0       0       0       0       0       0       0       9   \n",
       "1      1       0       0       0       0       0       0       0       0   \n",
       "2      2       0       0       0       0       0       0      14      53   \n",
       "3      2       0       0       0       0       0       0       0       0   \n",
       "4      3       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel9    ...     pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0       8    ...          103        87        56         0         0   \n",
       "1       0    ...           34         0         0         0         0   \n",
       "2      99    ...            0         0         0         0        63   \n",
       "3       0    ...          137       126       140         0       133   \n",
       "4       0    ...            0         0         0         0         0   \n",
       "\n",
       "   pixel780  pixel781  pixel782  pixel783  pixel784  \n",
       "0         0         0         0         0         0  \n",
       "1         0         0         0         0         0  \n",
       "2        53        31         0         0         0  \n",
       "3       224       222        56         0         0  \n",
       "4         0         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = pd.read_csv('fashion-mnist_test.csv')\n",
    "test_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_X = test_dataset[test_dataset.columns[1:]]\n",
    "test_Y = np.array(test_dataset[\"label\"])\n",
    "#print(test_X.values.max(),test_X.values.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_X = (test_X - test_X.values.mean()) / test_X.values.std() #Standardization of test_X dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_X = (test_X - test_X.values.min())/ (test_X.values.max() - test_X.values.min())  #Normalized_X = (X â€“ min)/(max-min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>percent_filled</th>\n",
       "      <th>percent_filled_top</th>\n",
       "      <th>percent_filled_bottom</th>\n",
       "      <th>row_sum_0</th>\n",
       "      <th>row_sum_1</th>\n",
       "      <th>row_sum_2</th>\n",
       "      <th>row_sum_3</th>\n",
       "      <th>row_sum_4</th>\n",
       "      <th>row_sum_5</th>\n",
       "      <th>row_sum_6</th>\n",
       "      <th>...</th>\n",
       "      <th>row_sum_19</th>\n",
       "      <th>row_sum_20</th>\n",
       "      <th>row_sum_21</th>\n",
       "      <th>row_sum_22</th>\n",
       "      <th>row_sum_23</th>\n",
       "      <th>row_sum_24</th>\n",
       "      <th>row_sum_25</th>\n",
       "      <th>row_sum_26</th>\n",
       "      <th>row_sum_27</th>\n",
       "      <th>symmetry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.399620</td>\n",
       "      <td>0.422389</td>\n",
       "      <td>0.376851</td>\n",
       "      <td>0.018347</td>\n",
       "      <td>0.078992</td>\n",
       "      <td>0.185994</td>\n",
       "      <td>0.414286</td>\n",
       "      <td>0.550840</td>\n",
       "      <td>0.540196</td>\n",
       "      <td>0.518347</td>\n",
       "      <td>...</td>\n",
       "      <td>0.388235</td>\n",
       "      <td>0.391737</td>\n",
       "      <td>0.395098</td>\n",
       "      <td>0.395938</td>\n",
       "      <td>0.400560</td>\n",
       "      <td>0.393697</td>\n",
       "      <td>0.370728</td>\n",
       "      <td>0.457843</td>\n",
       "      <td>0.212745</td>\n",
       "      <td>0.161990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.210664</td>\n",
       "      <td>0.236134</td>\n",
       "      <td>0.185194</td>\n",
       "      <td>0.209664</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.267647</td>\n",
       "      <td>0.291877</td>\n",
       "      <td>0.267787</td>\n",
       "      <td>0.248880</td>\n",
       "      <td>0.240756</td>\n",
       "      <td>...</td>\n",
       "      <td>0.199160</td>\n",
       "      <td>0.201821</td>\n",
       "      <td>0.199860</td>\n",
       "      <td>0.202941</td>\n",
       "      <td>0.198039</td>\n",
       "      <td>0.191317</td>\n",
       "      <td>0.185294</td>\n",
       "      <td>0.185294</td>\n",
       "      <td>0.155742</td>\n",
       "      <td>0.084184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.168382</td>\n",
       "      <td>0.193377</td>\n",
       "      <td>0.143387</td>\n",
       "      <td>0.051961</td>\n",
       "      <td>0.157563</td>\n",
       "      <td>0.267507</td>\n",
       "      <td>0.189076</td>\n",
       "      <td>0.192577</td>\n",
       "      <td>0.219328</td>\n",
       "      <td>0.205462</td>\n",
       "      <td>...</td>\n",
       "      <td>0.211345</td>\n",
       "      <td>0.240196</td>\n",
       "      <td>0.157423</td>\n",
       "      <td>0.139216</td>\n",
       "      <td>0.113165</td>\n",
       "      <td>0.079272</td>\n",
       "      <td>0.072129</td>\n",
       "      <td>0.028291</td>\n",
       "      <td>0.038375</td>\n",
       "      <td>0.010204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.407893</td>\n",
       "      <td>0.460554</td>\n",
       "      <td>0.355232</td>\n",
       "      <td>0.257143</td>\n",
       "      <td>0.455602</td>\n",
       "      <td>0.452101</td>\n",
       "      <td>0.430532</td>\n",
       "      <td>0.433053</td>\n",
       "      <td>0.462465</td>\n",
       "      <td>0.500560</td>\n",
       "      <td>...</td>\n",
       "      <td>0.374510</td>\n",
       "      <td>0.319888</td>\n",
       "      <td>0.264146</td>\n",
       "      <td>0.236835</td>\n",
       "      <td>0.277311</td>\n",
       "      <td>0.257423</td>\n",
       "      <td>0.475070</td>\n",
       "      <td>0.725350</td>\n",
       "      <td>0.445798</td>\n",
       "      <td>0.103316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.252716</td>\n",
       "      <td>0.325090</td>\n",
       "      <td>0.180342</td>\n",
       "      <td>0.007563</td>\n",
       "      <td>0.281092</td>\n",
       "      <td>0.360924</td>\n",
       "      <td>0.362745</td>\n",
       "      <td>0.369328</td>\n",
       "      <td>0.381933</td>\n",
       "      <td>0.343277</td>\n",
       "      <td>...</td>\n",
       "      <td>0.251261</td>\n",
       "      <td>0.259664</td>\n",
       "      <td>0.216527</td>\n",
       "      <td>0.034594</td>\n",
       "      <td>0.026751</td>\n",
       "      <td>0.017647</td>\n",
       "      <td>0.016387</td>\n",
       "      <td>0.010364</td>\n",
       "      <td>0.010364</td>\n",
       "      <td>0.079082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.270638</td>\n",
       "      <td>0.251210</td>\n",
       "      <td>0.290066</td>\n",
       "      <td>0.047479</td>\n",
       "      <td>0.181513</td>\n",
       "      <td>0.237255</td>\n",
       "      <td>0.223389</td>\n",
       "      <td>0.230952</td>\n",
       "      <td>0.228011</td>\n",
       "      <td>0.247899</td>\n",
       "      <td>...</td>\n",
       "      <td>0.271849</td>\n",
       "      <td>0.238515</td>\n",
       "      <td>0.256162</td>\n",
       "      <td>0.267647</td>\n",
       "      <td>0.272829</td>\n",
       "      <td>0.282353</td>\n",
       "      <td>0.298599</td>\n",
       "      <td>0.286975</td>\n",
       "      <td>0.188375</td>\n",
       "      <td>0.010204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.404432</td>\n",
       "      <td>0.368958</td>\n",
       "      <td>0.439906</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.395098</td>\n",
       "      <td>...</td>\n",
       "      <td>0.780532</td>\n",
       "      <td>0.792997</td>\n",
       "      <td>0.885714</td>\n",
       "      <td>0.123950</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.202806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.290756</td>\n",
       "      <td>0.255282</td>\n",
       "      <td>0.326230</td>\n",
       "      <td>0.045238</td>\n",
       "      <td>0.160784</td>\n",
       "      <td>0.236835</td>\n",
       "      <td>0.185854</td>\n",
       "      <td>0.223389</td>\n",
       "      <td>0.243417</td>\n",
       "      <td>0.255462</td>\n",
       "      <td>...</td>\n",
       "      <td>0.346359</td>\n",
       "      <td>0.344398</td>\n",
       "      <td>0.332773</td>\n",
       "      <td>0.324790</td>\n",
       "      <td>0.347059</td>\n",
       "      <td>0.280532</td>\n",
       "      <td>0.261485</td>\n",
       "      <td>0.311204</td>\n",
       "      <td>0.283193</td>\n",
       "      <td>0.052296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.167462</td>\n",
       "      <td>0.139776</td>\n",
       "      <td>0.195148</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030952</td>\n",
       "      <td>0.163165</td>\n",
       "      <td>0.316947</td>\n",
       "      <td>0.357843</td>\n",
       "      <td>0.205042</td>\n",
       "      <td>0.085294</td>\n",
       "      <td>...</td>\n",
       "      <td>0.124790</td>\n",
       "      <td>0.178571</td>\n",
       "      <td>0.183333</td>\n",
       "      <td>0.178011</td>\n",
       "      <td>0.339916</td>\n",
       "      <td>0.399860</td>\n",
       "      <td>0.423249</td>\n",
       "      <td>0.176331</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.215581</td>\n",
       "      <td>0.257233</td>\n",
       "      <td>0.173930</td>\n",
       "      <td>0.046218</td>\n",
       "      <td>0.245238</td>\n",
       "      <td>0.248319</td>\n",
       "      <td>0.227731</td>\n",
       "      <td>0.203221</td>\n",
       "      <td>0.267227</td>\n",
       "      <td>0.389776</td>\n",
       "      <td>...</td>\n",
       "      <td>0.175350</td>\n",
       "      <td>0.173389</td>\n",
       "      <td>0.176611</td>\n",
       "      <td>0.185014</td>\n",
       "      <td>0.180952</td>\n",
       "      <td>0.190616</td>\n",
       "      <td>0.192297</td>\n",
       "      <td>0.224090</td>\n",
       "      <td>0.107423</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   percent_filled  percent_filled_top  percent_filled_bottom  row_sum_0  \\\n",
       "0        0.399620            0.422389               0.376851   0.018347   \n",
       "1        0.210664            0.236134               0.185194   0.209664   \n",
       "2        0.168382            0.193377               0.143387   0.051961   \n",
       "3        0.407893            0.460554               0.355232   0.257143   \n",
       "4        0.252716            0.325090               0.180342   0.007563   \n",
       "5        0.270638            0.251210               0.290066   0.047479   \n",
       "6        0.404432            0.368958               0.439906   0.000000   \n",
       "7        0.290756            0.255282               0.326230   0.045238   \n",
       "8        0.167462            0.139776               0.195148   0.000000   \n",
       "9        0.215581            0.257233               0.173930   0.046218   \n",
       "\n",
       "   row_sum_1  row_sum_2  row_sum_3  row_sum_4  row_sum_5  row_sum_6    ...     \\\n",
       "0   0.078992   0.185994   0.414286   0.550840   0.540196   0.518347    ...      \n",
       "1   0.250000   0.267647   0.291877   0.267787   0.248880   0.240756    ...      \n",
       "2   0.157563   0.267507   0.189076   0.192577   0.219328   0.205462    ...      \n",
       "3   0.455602   0.452101   0.430532   0.433053   0.462465   0.500560    ...      \n",
       "4   0.281092   0.360924   0.362745   0.369328   0.381933   0.343277    ...      \n",
       "5   0.181513   0.237255   0.223389   0.230952   0.228011   0.247899    ...      \n",
       "6   0.000000   0.000000   0.000000   0.000000   0.000000   0.395098    ...      \n",
       "7   0.160784   0.236835   0.185854   0.223389   0.243417   0.255462    ...      \n",
       "8   0.030952   0.163165   0.316947   0.357843   0.205042   0.085294    ...      \n",
       "9   0.245238   0.248319   0.227731   0.203221   0.267227   0.389776    ...      \n",
       "\n",
       "   row_sum_19  row_sum_20  row_sum_21  row_sum_22  row_sum_23  row_sum_24  \\\n",
       "0    0.388235    0.391737    0.395098    0.395938    0.400560    0.393697   \n",
       "1    0.199160    0.201821    0.199860    0.202941    0.198039    0.191317   \n",
       "2    0.211345    0.240196    0.157423    0.139216    0.113165    0.079272   \n",
       "3    0.374510    0.319888    0.264146    0.236835    0.277311    0.257423   \n",
       "4    0.251261    0.259664    0.216527    0.034594    0.026751    0.017647   \n",
       "5    0.271849    0.238515    0.256162    0.267647    0.272829    0.282353   \n",
       "6    0.780532    0.792997    0.885714    0.123950    0.000000    0.000000   \n",
       "7    0.346359    0.344398    0.332773    0.324790    0.347059    0.280532   \n",
       "8    0.124790    0.178571    0.183333    0.178011    0.339916    0.399860   \n",
       "9    0.175350    0.173389    0.176611    0.185014    0.180952    0.190616   \n",
       "\n",
       "   row_sum_25  row_sum_26  row_sum_27  symmetry  \n",
       "0    0.370728    0.457843    0.212745  0.161990  \n",
       "1    0.185294    0.185294    0.155742  0.084184  \n",
       "2    0.072129    0.028291    0.038375  0.010204  \n",
       "3    0.475070    0.725350    0.445798  0.103316  \n",
       "4    0.016387    0.010364    0.010364  0.079082  \n",
       "5    0.298599    0.286975    0.188375  0.010204  \n",
       "6    0.000000    0.000000    0.000000  0.202806  \n",
       "7    0.261485    0.311204    0.283193  0.052296  \n",
       "8    0.423249    0.176331    0.000000  0.033163  \n",
       "9    0.192297    0.224090    0.107423  0.000000  \n",
       "\n",
       "[10 rows x 32 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "engineered_features_test = pd.DataFrame()\n",
    "\n",
    "# Calcualte percentage of filled pixels and a top and bottom half only version\n",
    "percent_filled = test_X.sum(axis = 1)/(28*28)\n",
    "percent_filled_top = test_X.iloc[:, 0:392].sum(axis = 1)/(28*14)\n",
    "percent_filled_bottom = test_X.iloc[:, 392:784].sum(axis = 1)/(28*14)\n",
    "engineered_features_test['percent_filled'] = percent_filled\n",
    "engineered_features_test['percent_filled_top'] = percent_filled_top\n",
    "engineered_features_test['percent_filled_bottom'] = percent_filled_bottom\n",
    "\n",
    "# Calculate the sum of each row\n",
    "for idx, i in enumerate(range(0, 784, 28)):\n",
    "    row_sum = test_X.iloc[:, i:(i + 28)].sum(axis = 1)/28\n",
    "    engineered_features_test[\"row_sum_\" + str(idx)] = row_sum\n",
    "\n",
    "# Calcualte a measure of syymmetry around a horizontal axis\n",
    "s1 = np.round(np.array(test_X.iloc[:, 0:392]))\n",
    "s2 = np.round(np.array(test_X.iloc[:, 784:391:-1]))\n",
    "s3 = np.logical_and(s1, s2).astype(int)\n",
    "symmetry = s3.sum(axis = 1)/(28*28)\n",
    "\n",
    "engineered_features_test['symmetry'] = symmetry\n",
    "\n",
    "display(engineered_features_test.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_X = engineered_features_test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate a Simple Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a Super Learner Classifier using the prepared dataset (on the training part of the train data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SuperLearnerClassifier(add_original_input=False,\n",
       "            base_classifiers=[DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=6,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=200,\n",
       "            min_weight_fraction_lea...b_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False), GaussianNB(priors=None)],\n",
       "            base_clsf_type=0, cv=3, probability_outputs=False,\n",
       "            shuffle=True,\n",
       "            stacked_layer_classifier=DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=6,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=200,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best'),\n",
       "            stratify=True, type_of_model=0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "clf = SuperLearnerClassifier()\n",
    "#clf.fit(X_train,y_train)\n",
    "clf.fit(np.array(X_train),np.array(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the trained classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assess the performance of the Super Learner Classifier on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.828666666667\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.79      0.79      0.79       140\n",
      "          1       0.83      0.92      0.87       156\n",
      "          2       0.79      0.93      0.85       153\n",
      "          3       0.99      0.85      0.92       141\n",
      "          4       0.97      0.95      0.96       141\n",
      "          5       0.97      0.76      0.85       173\n",
      "          6       0.61      0.75      0.67       151\n",
      "          7       0.74      0.97      0.84       153\n",
      "          8       0.84      0.84      0.84       153\n",
      "          9       0.99      0.51      0.67       139\n",
      "\n",
      "avg / total       0.85      0.83      0.83      1500\n",
      "\n",
      "[[111   5  23   0   0   1   0   0   0   0]\n",
      " [  1 143  12   0   0   0   0   0   0   0]\n",
      " [  1   5 143   1   0   0   0   0   3   0]\n",
      " [ 12   6   2 120   0   0   0   0   1   0]\n",
      " [  0   1   0   0 134   0   1   0   5   0]\n",
      " [  0   1   0   0   0 132  32   0   7   1]\n",
      " [ 15  12   1   0   4   1 113   0   5   0]\n",
      " [  0   0   0   0   0   2   0 148   3   0]\n",
      " [  0   0   0   0   0   0  17   8 128   0]\n",
      " [  0   0   1   0   0   0  22  45   0  71]]\n",
      "Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>111</td>\n",
       "      <td>5</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>143</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>143</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>134</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>132</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>113</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>148</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>71</td>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>140</td>\n",
       "      <td>173</td>\n",
       "      <td>182</td>\n",
       "      <td>121</td>\n",
       "      <td>138</td>\n",
       "      <td>136</td>\n",
       "      <td>185</td>\n",
       "      <td>201</td>\n",
       "      <td>152</td>\n",
       "      <td>72</td>\n",
       "      <td>1500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted    0    1    2    3    4    5    6    7    8   9   All\n",
       "True                                                            \n",
       "0          111    5   23    0    0    1    0    0    0   0   140\n",
       "1            1  143   12    0    0    0    0    0    0   0   156\n",
       "2            1    5  143    1    0    0    0    0    3   0   153\n",
       "3           12    6    2  120    0    0    0    0    1   0   141\n",
       "4            0    1    0    0  134    0    1    0    5   0   141\n",
       "5            0    1    0    0    0  132   32    0    7   1   173\n",
       "6           15   12    1    0    4    1  113    0    5   0   151\n",
       "7            0    0    0    0    0    2    0  148    3   0   153\n",
       "8            0    0    0    0    0    0   17    8  128   0   153\n",
       "9            0    0    1    0    0    0   22   45    0  71   139\n",
       "All        140  173  182  121  138  136  185  201  152  72  1500"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a set of predictions for the training data\n",
    "y_pred = clf.predict(X_train)\n",
    "\n",
    "# Print performance details\n",
    "accuracy = metrics.accuracy_score(y_train, y_pred) #normalize=True, \n",
    "                                                #sample_weight=None\n",
    "print(\"Accuracy: \" +  str(accuracy))\n",
    "print(metrics.classification_report(y_train, y_pred))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(metrics.confusion_matrix(y_train, y_pred))\n",
    "\n",
    "# Print nicer homemade confusion matrix\n",
    "print(\"Confusion Matrix\")\n",
    "pd.crosstab(np.array(y_train), y_pred, \n",
    "            rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assess the performance of the Super Learner Classifier on the validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.608333333333\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.75      0.62      0.68        64\n",
      "          1       0.76      0.91      0.83        64\n",
      "          2       0.34      0.58      0.43        52\n",
      "          3       0.61      0.46      0.53        71\n",
      "          4       0.61      0.60      0.60        67\n",
      "          5       0.78      0.57      0.66        61\n",
      "          6       0.16      0.20      0.18        45\n",
      "          7       0.60      0.89      0.72        53\n",
      "          8       0.86      0.77      0.81        57\n",
      "          9       0.91      0.44      0.59        66\n",
      "\n",
      "avg / total       0.66      0.61      0.61       600\n",
      "\n",
      "[[40  3 13  3  1  0  4  0  0  0]\n",
      " [ 0 58  2  3  1  0  0  0  0  0]\n",
      " [ 1  3 30  1 10  0  5  0  2  0]\n",
      " [ 7  8  9 33  3  0 11  0  0  0]\n",
      " [ 0  2 14  6 40  0  4  0  1  0]\n",
      " [ 0  0  2  2  0 35 14  6  1  1]\n",
      " [ 5  2 14  5  9  0  9  0  0  1]\n",
      " [ 0  0  0  0  0  6  0 47  0  0]\n",
      " [ 0  0  2  0  1  2  4  3 44  1]\n",
      " [ 0  0  2  1  1  2  6 22  3 29]]\n",
      "Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>33</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>29</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>53</td>\n",
       "      <td>76</td>\n",
       "      <td>88</td>\n",
       "      <td>54</td>\n",
       "      <td>66</td>\n",
       "      <td>45</td>\n",
       "      <td>57</td>\n",
       "      <td>78</td>\n",
       "      <td>51</td>\n",
       "      <td>32</td>\n",
       "      <td>600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted   0   1   2   3   4   5   6   7   8   9  All\n",
       "True                                                  \n",
       "0          40   3  13   3   1   0   4   0   0   0   64\n",
       "1           0  58   2   3   1   0   0   0   0   0   64\n",
       "2           1   3  30   1  10   0   5   0   2   0   52\n",
       "3           7   8   9  33   3   0  11   0   0   0   71\n",
       "4           0   2  14   6  40   0   4   0   1   0   67\n",
       "5           0   0   2   2   0  35  14   6   1   1   61\n",
       "6           5   2  14   5   9   0   9   0   0   1   45\n",
       "7           0   0   0   0   0   6   0  47   0   0   53\n",
       "8           0   0   2   0   1   2   4   3  44   1   57\n",
       "9           0   0   2   1   1   2   6  22   3  29   66\n",
       "All        53  76  88  54  66  45  57  78  51  32  600"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a set of predictions for the validation data\n",
    "y_pred = clf.predict(X_valid)\n",
    "\n",
    "# Print performance details\n",
    "accuracy = metrics.accuracy_score(y_valid, y_pred) #normalize=True, \n",
    "                                                #sample_weight=None\n",
    "\n",
    "    \n",
    "model_valid_accuracy_comparisons[\"Default Super Learner\"] = accuracy\n",
    "\n",
    "print(\"Accuracy: \" + str(accuracy))\n",
    "print(metrics.classification_report(y_valid, y_pred))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(metrics.confusion_matrix(y_valid, y_pred))\n",
    "\n",
    "# Print nicer confusion matrix\n",
    "print(\"Confusion Matrix\")\n",
    "pd.crosstab(np.array(y_valid), y_pred, rownames=['True'], \n",
    "            colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assess the performance of the Super Learner Classifier on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.616666666667\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.68      0.60      0.64        83\n",
      "          1       0.77      0.89      0.83       101\n",
      "          2       0.49      0.71      0.58        85\n",
      "          3       0.53      0.47      0.50        81\n",
      "          4       0.55      0.61      0.58        84\n",
      "          5       0.71      0.55      0.62        92\n",
      "          6       0.28      0.32      0.30        90\n",
      "          7       0.60      0.78      0.68        87\n",
      "          8       0.86      0.71      0.78        93\n",
      "          9       0.91      0.50      0.65       104\n",
      "\n",
      "avg / total       0.65      0.62      0.62       900\n",
      "\n",
      "[[50 10  8  6  3  0  5  0  1  0]\n",
      " [ 1 90  3  7  0  0  0  0  0  0]\n",
      " [ 3  2 60  3 11  0  5  0  0  1]\n",
      " [ 8  7 17 38  4  0  7  0  0  0]\n",
      " [ 0  2 11  5 51  0 12  0  3  0]\n",
      " [ 0  0  2  3  0 51 20 14  1  1]\n",
      " [11  5 16  6 21  0 29  0  2  0]\n",
      " [ 0  0  0  0  0 17  0 68  0  2]\n",
      " [ 0  0  2  0  2  0 14  8 66  1]\n",
      " [ 0  1  3  4  1  4 11 24  4 52]]\n",
      "Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>90</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>60</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>17</td>\n",
       "      <td>38</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>51</td>\n",
       "      <td>20</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>66</td>\n",
       "      <td>1</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>24</td>\n",
       "      <td>4</td>\n",
       "      <td>52</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>73</td>\n",
       "      <td>117</td>\n",
       "      <td>122</td>\n",
       "      <td>72</td>\n",
       "      <td>93</td>\n",
       "      <td>72</td>\n",
       "      <td>103</td>\n",
       "      <td>114</td>\n",
       "      <td>77</td>\n",
       "      <td>57</td>\n",
       "      <td>900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted   0    1    2   3   4   5    6    7   8   9  All\n",
       "True                                                      \n",
       "0          50   10    8   6   3   0    5    0   1   0   83\n",
       "1           1   90    3   7   0   0    0    0   0   0  101\n",
       "2           3    2   60   3  11   0    5    0   0   1   85\n",
       "3           8    7   17  38   4   0    7    0   0   0   81\n",
       "4           0    2   11   5  51   0   12    0   3   0   84\n",
       "5           0    0    2   3   0  51   20   14   1   1   92\n",
       "6          11    5   16   6  21   0   29    0   2   0   90\n",
       "7           0    0    0   0   0  17    0   68   0   2   87\n",
       "8           0    0    2   0   2   0   14    8  66   1   93\n",
       "9           0    1    3   4   1   4   11   24   4  52  104\n",
       "All        73  117  122  72  93  72  103  114  77  57  900"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a set of predictions for the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Print performance details\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred) #normalize=True, \n",
    "                                            #sample_weight=None\n",
    "\n",
    "model_test_accuracy_comparisons[\"Default Super Learner\"] = accuracy    \n",
    "    \n",
    "print(\"Accuracy: \" +  str(accuracy))\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(metrics.confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Print nicer confusion matrix\n",
    "print(\"Confusion Matrix\")\n",
    "pd.crosstab(np.array(y_test), y_pred, \n",
    "            rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ----> Train a Super Learner Classifier using the whole prepared mnist training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nnp.random.seed(42)\\nclf = SuperLearnerClassifier()\\n#clf.fit(X,Y)\\nclf.fit(np.array(X),np.array(Y))\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "np.random.seed(42)\n",
    "clf = SuperLearnerClassifier()\n",
    "#clf.fit(X,Y)\n",
    "clf.fit(np.array(X),np.array(Y))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the trained classifier on the mnist test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Make a set of predictions for the test data\\ny_pred = clf.predict(test_X)\\n\\n# Print performance details\\naccuracy = metrics.accuracy_score(test_Y, y_pred) #normalize=True,\\n                                                #sample_weight=None\\nprint(\"Accuracy: \" +  str(accuracy))\\nprint(metrics.classification_report(test_Y, y_pred))\\n\\n# Print confusion matrix\\nprint(\"Confusion Matrix\")\\npd.crosstab(np.array(test_Y), y_pred, \\n            rownames=[\\'True\\'], colnames=[\\'Predicted\\'], margins=True)\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Make a set of predictions for the test data\n",
    "y_pred = clf.predict(test_X)\n",
    "\n",
    "# Print performance details\n",
    "accuracy = metrics.accuracy_score(test_Y, y_pred) #normalize=True,\n",
    "                                                #sample_weight=None\n",
    "print(\"Accuracy: \" +  str(accuracy))\n",
    "print(metrics.classification_report(test_Y, y_pred))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix\")\n",
    "pd.crosstab(np.array(test_Y), y_pred, \n",
    "            rownames=['True'], colnames=['Predicted'], margins=True)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation Experiment (Task 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification accuracy is the number of correct predictions made as a ratio of all predictions made.\n",
    "Most common evaluation metric for classification problems\n",
    "This metric is suitable for fashion mnist data because:\n",
    "\n",
    "    --> there is an equal number of observations in each class \n",
    "            (6000 for each of the 10 labels - check Examining the distribution of the ten classes:\n",
    "            dataset[\"label\"].value_counts()) \n",
    "    ---> all predictions and prediction errors are equally important (it is equaly important to us to correctly \n",
    "        predict Shirts as any other class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perfrom a 10-fold cross validation experiment to evaluate the performance of the SuperLearnerClassifier - on the train plus validation data from mnist train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(clf, np.array(X_train_plus_valid), np.array(y_train_plus_valid), cv=10, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ---> Perfrom a 10-fold cross validation experiment to evaluate the performance of the SuperLearnerClassifier on the entire mnist train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cross_val_score(clf, np.array(X), np.array(Y), cv=10, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing the Performance of Different Stack Layer Approaches (Task 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stack_model_name = [\n",
    "            'Decision Tree (label outputs)',\n",
    "            'Decision Tree (probability outputs)',\n",
    "    \n",
    "            'Logistic Regression (label outputs)',\n",
    "            'Logistic Regression (probability outputs)',\n",
    "    \n",
    "            'Support Vector Machine (label outputs)',\n",
    "            'Support Vector Machine (probability outputs)',\n",
    "    \n",
    "            'Nearest Neighbour (label outputs)',\n",
    "            'Nearest Neighbour (probability outputs)',\n",
    "    \n",
    "            'Random Forest (label outputs)',\n",
    "            'Random Forest (probability outputs)',\n",
    "    \n",
    "            'Naive Bayes (label outputs)',\n",
    "            'Naive Bayes (probability outputs)',\n",
    "    \n",
    "            'Multy Layer Perception (label outputs)',\n",
    "            'Multy Layer Perception (probability outputs)',\n",
    "    \n",
    "            'Bagging (label outputs)',\n",
    "            'Bagging (probability outputs)',\n",
    "    \n",
    "            'Ada Boost (label outputs)',\n",
    "            'Ada Boost (probability outputs)',\n",
    "    \n",
    "            'Gradient Boosting (label outputs)',\n",
    "            'Gradient Boosting (probability outputs)'\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Creating a list of super learners (different models types for stack layer classifier, \n",
    "#one trained on label outputs and one on probability outputs)\n",
    "super_learner_list=[]\n",
    "for i in range(10):\n",
    "    super_learner_list.append(SuperLearnerClassifier(type_of_model=i, probability_outputs=False))\n",
    "    super_learner_list.append(SuperLearnerClassifier(type_of_model=i, probability_outputs=True))\n",
    "#print(super_learner_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#creating empty dictionary that will contain different stacked layer model types for keys (with probability/label outputs info) \n",
    "#and mean accuracies of a super learner that uses that stacked layer model type for values\n",
    "sl_stack_model_accuracy_comparisons = dict() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree (label outputs) stack model 10-fold cross validation scores:\n",
      " [ 0.61682243  0.6056338   0.62441315  0.63849765  0.60663507  0.59330144\n",
      "  0.61244019  0.63768116  0.6407767   0.66341463]\n",
      "Accuracy (average score):  0.62 (+/- 0.02) ---> Decision Tree (label outputs) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "#checking the accurancy of the Super Learner for different stack layer model types \n",
    "for super_learner, stack_name in zip(super_learner_list, stack_model_name):\n",
    "    scores = cross_val_score(super_learner, np.array(X_train_plus_valid), np.array(y_train_plus_valid), cv=10, scoring='accuracy')\n",
    "    sl_stack_model_accuracy_comparisons[stack_name]=scores.mean()\n",
    "    print(stack_name, 'stack model 10-fold cross validation scores:\\n', scores)\n",
    "    print(\"Accuracy (average score):  %0.2f (+/- %0.2f) ---> %s\" \n",
    "          % (scores.mean(), scores.std(), stack_name),'\\n')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the performance of the ensemble when a label based stack layer training set and a probability based stack layer training set is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(sl_stack_model_accuracy_comparisons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAk0AAAD8CAYAAABq85ChAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a15767f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.xlim(0, 1.0)\n",
    "_ = plt.barh(range(len(sl_stack_model_accuracy_comparisons)), \n",
    "             list(sl_stack_model_accuracy_comparisons.values()), \n",
    "             align='center')\n",
    "_ = plt.yticks(range(len(sl_stack_model_accuracy_comparisons)), \n",
    "               list(sl_stack_model_accuracy_comparisons.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search Through SuperLearnerClassifier Architectures & Parameters (Task 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a grid search through a large set of possible parameters. \n",
    "Here we try all 10 different stack layer models, \n",
    "45 different base estimators combinations, and\n",
    "2 different stack layer training data types (with probability outputs and label outputs)\n",
    "Also, fitting cv = 2 folds for each of 900=10*45*2 candidates, totalling in 1800 fits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfrom a grid search experiment to detemrine the optimal architecture and hyper-parameter values for the SuperLearnClasssifier for the MNIST Fashion classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "# Set up the parameter grid to seaerch\n",
    "param_grid ={'type_of_model': list(range(0, 10)),\\\n",
    "             'base_clsf_type': list(range(0, 45)),\\\n",
    "             'probability_outputs': ['False', 'True']}\n",
    "\n",
    "# Perform the search\n",
    "my_tuned_superlearner = GridSearchCV(SuperLearnerClassifier(), param_grid, cv=2, verbose = 2, return_train_score=True)\n",
    "my_tuned_superlearner.fit(np.array(X_train_plus_valid), np.array(y_train_plus_valid))\n",
    "\n",
    "# Print details\n",
    "print(\"Best parameters set found on development set:\")\n",
    "display(my_tuned_superlearner.best_params_) #Parameter setting that gave the best results on the hold out data.\n",
    "\n",
    "model_tuned_params_list[\"Tuned Super Learner\"] = my_tuned_superlearner.best_params_\n",
    "\n",
    "display(my_tuned_superlearner.best_score_) #Mean cross-validated score of the best_estimator\n",
    "display(my_tuned_superlearner.cv_results_) #A dict with keys as column headers and values as columns, that can be imported into a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Tuned Super Learner': {'base_clsf_type': 27, 'probability_outputs': 'True', 'type_of_model': 1}}\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Best parameters set found on development set:\n",
    "{'base_clsf_type': 27, 'probability_outputs': 'True', 'type_of_model': 1}\n",
    "\n",
    "0.70428571428571429\n",
    "'''\n",
    "print(model_tuned_params_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the performance of the model selected by the grid search on a hold-out dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.692222222222\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.62      0.67      0.64        90\n",
      "          1       0.86      0.79      0.82        91\n",
      "          2       0.52      0.56      0.54        84\n",
      "          3       0.58      0.71      0.64        98\n",
      "          4       0.60      0.56      0.58       110\n",
      "          5       0.84      0.84      0.84        82\n",
      "          6       0.49      0.38      0.43        97\n",
      "          7       0.87      0.82      0.85       100\n",
      "          8       0.87      0.90      0.89        73\n",
      "          9       0.75      0.77      0.76        75\n",
      "\n",
      "avg / total       0.69      0.69      0.69       900\n",
      "\n",
      "Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>72</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>47</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>70</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>9</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>66</td>\n",
       "      <td>2</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>97</td>\n",
       "      <td>84</td>\n",
       "      <td>90</td>\n",
       "      <td>120</td>\n",
       "      <td>104</td>\n",
       "      <td>82</td>\n",
       "      <td>76</td>\n",
       "      <td>94</td>\n",
       "      <td>76</td>\n",
       "      <td>77</td>\n",
       "      <td>900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted   0   1   2    3    4   5   6   7   8   9  All\n",
       "True                                                    \n",
       "0          60   4   3   10    3   1   7   0   2   0   90\n",
       "1           2  72   1   14    0   0   2   0   0   0   91\n",
       "2           5   2  47    2   19   0   8   0   1   0   84\n",
       "3          13   5   2   70    2   1   4   0   1   0   98\n",
       "4           3   0  21    9   62   0  14   0   0   1  110\n",
       "5           0   0   0    1    0  69   2   5   1   4   82\n",
       "6          14   1  10   13   18   0  37   0   3   1   97\n",
       "7           0   0   0    0    0   6   0  82   1  11  100\n",
       "8           0   0   3    0    0   1   0   1  66   2   73\n",
       "9           0   0   3    1    0   4   2   6   1  58   75\n",
       "All        97  84  90  120  104  82  76  94  76  77  900"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "# Make a set of predictions for the test data\n",
    "y_pred = my_tuned_superlearner.predict(X_test) #X_test - hold-out dataset\n",
    "\n",
    "# Print performance details\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred) #normalize=True,\n",
    "                                                #sample_weight=None\n",
    "\n",
    "model_test_accuracy_comparisons[\"Tuned Super Learner\"] = accuracy\n",
    "\n",
    "print(\"Accuracy: \" +  str(accuracy))\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix\")\n",
    "pd.crosstab(np.array(y_test), y_pred, \n",
    "            rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Default Super Learner': 0.6333333333333333}\n"
     ]
    }
   ],
   "source": [
    "print(model_valid_accuracy_comparisons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Default Super Learner': 0.60333333333333339, 'Tuned Super Learner': 0.69222222222222218}\n"
     ]
    }
   ],
   "source": [
    "print(model_test_accuracy_comparisons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Tuned Super Learner': {'base_clsf_type': 27, 'probability_outputs': 'True', 'type_of_model': 1}}\n"
     ]
    }
   ],
   "source": [
    "print(model_tuned_params_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Impact of Adding Original Descriptive Features at the Stack Layer (Task 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the impact of adding original descriptive features at the stack layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SuperLearnerClassifier(add_original_input=True,\n",
       "            base_classifiers=[DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=6,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=200,\n",
       "            min_weight_fraction_lea...b_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False), GaussianNB(priors=None)],\n",
       "            base_clsf_type=35, cv=3, probability_outputs=False,\n",
       "            shuffle=True,\n",
       "            stacked_layer_classifier=DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=6,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=200,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best'),\n",
       "            stratify=True, type_of_model=1)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "#Using the best setup found in Task 7 and adding original descriptive features at the stack layer.\n",
    "'''\n",
    "Best parameters set found on development set:\n",
    "{'base_clsf_type': 27, 'probability_outputs': 'True', 'type_of_model': 1}\n",
    "\n",
    "0.70428571428571429\n",
    "'''\n",
    "my_best_superlearner = SuperLearnerClassifier(type_of_model=1, base_clsf_type=35, probability_outputs=False, add_original_input=True)\n",
    "my_best_superlearner.fit(np.array(X_train),np.array(y_train))  #training best setup supe learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.698666666667\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.70      0.68      0.69       151\n",
      "          1       0.70      0.90      0.79       146\n",
      "          2       0.62      0.57      0.59       148\n",
      "          3       0.61      0.62      0.62       168\n",
      "          4       0.61      0.56      0.59       140\n",
      "          5       0.67      0.72      0.69       137\n",
      "          6       0.56      0.44      0.49       142\n",
      "          7       0.78      0.83      0.80       158\n",
      "          8       0.88      0.78      0.83       136\n",
      "          9       0.81      0.84      0.83       174\n",
      "\n",
      "avg / total       0.70      0.70      0.69      1500\n",
      "\n",
      "[[103  22   4  13   3   1   5   0   0   0]\n",
      " [  3 132   0   9   1   0   1   0   0   0]\n",
      " [  5  11  85  14  21   2  10   0   0   0]\n",
      " [ 12  14  11 105   7   7  12   0   0   0]\n",
      " [  2   5  31   8  79   2  13   0   0   0]\n",
      " [  1   0   2   6   0  98   4  21   0   5]\n",
      " [ 19   4   5  15  15  10  62   1   6   5]\n",
      " [  0   0   0   0   0  21   0 131   0   6]\n",
      " [  0   0   0   0   4   2   1   4 106  19]\n",
      " [  2   0   0   1   0   3   2  11   8 147]]\n",
      "Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>103</td>\n",
       "      <td>22</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>132</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>85</td>\n",
       "      <td>14</td>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>11</td>\n",
       "      <td>105</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>31</td>\n",
       "      <td>8</td>\n",
       "      <td>79</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>98</td>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>10</td>\n",
       "      <td>62</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>131</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>106</td>\n",
       "      <td>19</td>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>147</td>\n",
       "      <td>174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>147</td>\n",
       "      <td>188</td>\n",
       "      <td>138</td>\n",
       "      <td>171</td>\n",
       "      <td>130</td>\n",
       "      <td>146</td>\n",
       "      <td>110</td>\n",
       "      <td>168</td>\n",
       "      <td>120</td>\n",
       "      <td>182</td>\n",
       "      <td>1500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted    0    1    2    3    4    5    6    7    8    9   All\n",
       "True                                                             \n",
       "0          103   22    4   13    3    1    5    0    0    0   151\n",
       "1            3  132    0    9    1    0    1    0    0    0   146\n",
       "2            5   11   85   14   21    2   10    0    0    0   148\n",
       "3           12   14   11  105    7    7   12    0    0    0   168\n",
       "4            2    5   31    8   79    2   13    0    0    0   140\n",
       "5            1    0    2    6    0   98    4   21    0    5   137\n",
       "6           19    4    5   15   15   10   62    1    6    5   142\n",
       "7            0    0    0    0    0   21    0  131    0    6   158\n",
       "8            0    0    0    0    4    2    1    4  106   19   136\n",
       "9            2    0    0    1    0    3    2   11    8  147   174\n",
       "All        147  188  138  171  130  146  110  168  120  182  1500"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a set of predictions for the training data\n",
    "y_pred = my_best_superlearner.predict(X_train)\n",
    "\n",
    "# Print performance details  y_train_plus_valid  \n",
    "accuracy = metrics.accuracy_score(y_train, y_pred) #normalize=True, \n",
    "                                                #sample_weight=None\n",
    "print(\"Accuracy: \" +  str(accuracy))\n",
    "print(metrics.classification_report(y_train, y_pred))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(metrics.confusion_matrix(y_train, y_pred))\n",
    "\n",
    "# Print nicer homemade confusion matrix\n",
    "print(\"Confusion Matrix\")\n",
    "pd.crosstab(np.array(y_train), y_pred, \n",
    "            rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.633333333333\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.65      0.56      0.60        59\n",
      "          1       0.67      0.88      0.76        59\n",
      "          2       0.51      0.52      0.52        69\n",
      "          3       0.54      0.52      0.53        69\n",
      "          4       0.45      0.47      0.46        51\n",
      "          5       0.68      0.59      0.63        51\n",
      "          6       0.40      0.37      0.38        63\n",
      "          7       0.83      0.88      0.85        66\n",
      "          8       0.87      0.69      0.77        58\n",
      "          9       0.75      0.87      0.81        55\n",
      "\n",
      "avg / total       0.63      0.63      0.63       600\n",
      "\n",
      "[[33  9  0  7  2  0  5  0  2  1]\n",
      " [ 1 52  3  3  0  0  0  0  0  0]\n",
      " [ 3  4 36  4 13  1  8  0  0  0]\n",
      " [ 5  5  9 36  4  3  7  0  0  0]\n",
      " [ 1  1 11  2 24  0 11  0  0  1]\n",
      " [ 0  0  0  6  0 30  0 10  1  4]\n",
      " [ 7  6  7  9  8  2 23  0  1  0]\n",
      " [ 0  0  0  0  0  6  0 58  0  2]\n",
      " [ 1  1  3  0  2  0  2  1 40  8]\n",
      " [ 0  0  1  0  0  2  1  1  2 48]]\n",
      "Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>52</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>36</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>36</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>8</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>48</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>51</td>\n",
       "      <td>78</td>\n",
       "      <td>70</td>\n",
       "      <td>67</td>\n",
       "      <td>53</td>\n",
       "      <td>44</td>\n",
       "      <td>57</td>\n",
       "      <td>70</td>\n",
       "      <td>46</td>\n",
       "      <td>64</td>\n",
       "      <td>600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted   0   1   2   3   4   5   6   7   8   9  All\n",
       "True                                                  \n",
       "0          33   9   0   7   2   0   5   0   2   1   59\n",
       "1           1  52   3   3   0   0   0   0   0   0   59\n",
       "2           3   4  36   4  13   1   8   0   0   0   69\n",
       "3           5   5   9  36   4   3   7   0   0   0   69\n",
       "4           1   1  11   2  24   0  11   0   0   1   51\n",
       "5           0   0   0   6   0  30   0  10   1   4   51\n",
       "6           7   6   7   9   8   2  23   0   1   0   63\n",
       "7           0   0   0   0   0   6   0  58   0   2   66\n",
       "8           1   1   3   0   2   0   2   1  40   8   58\n",
       "9           0   0   1   0   0   2   1   1   2  48   55\n",
       "All        51  78  70  67  53  44  57  70  46  64  600"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Make a set of predictions for the validation data\n",
    "y_pred = my_best_superlearner.predict(X_valid)\n",
    "\n",
    "# Print performance details\n",
    "accuracy = metrics.accuracy_score(y_valid, y_pred) #normalize=True, \n",
    "                                                #sample_weight=None\n",
    "\n",
    "    \n",
    "model_valid_accuracy_comparisons[\"Best Setup Super Learner\"] = accuracy\n",
    "\n",
    "print(\"Accuracy: \" + str(accuracy))\n",
    "print(metrics.classification_report(y_valid, y_pred))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(metrics.confusion_matrix(y_valid, y_pred))\n",
    "\n",
    "# Print nicer confusion matrix\n",
    "print(\"Confusion Matrix\")\n",
    "pd.crosstab(np.array(y_valid), y_pred, rownames=['True'], \n",
    "            colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.622222222222\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.58      0.51      0.54        90\n",
      "          1       0.67      0.81      0.74        91\n",
      "          2       0.45      0.45      0.45        84\n",
      "          3       0.51      0.64      0.57        98\n",
      "          4       0.57      0.46      0.51       110\n",
      "          5       0.72      0.77      0.74        82\n",
      "          6       0.46      0.36      0.40        97\n",
      "          7       0.83      0.79      0.81       100\n",
      "          8       0.77      0.75      0.76        73\n",
      "          9       0.67      0.75      0.71        75\n",
      "\n",
      "avg / total       0.62      0.62      0.62       900\n",
      "\n",
      "[[46 16  2 15  2  2  4  0  3  0]\n",
      " [ 4 74  3  8  1  0  1  0  0  0]\n",
      " [ 2  8 38  7 18  1  8  0  2  0]\n",
      " [12  5  8 63  1  1  6  0  2  0]\n",
      " [ 2  2 22 10 51  1 19  0  2  1]\n",
      " [ 0  0  0  4  0 63  1  7  0  7]\n",
      " [12  5  9 14 15  2 35  0  2  3]\n",
      " [ 0  0  0  0  0  9  0 79  1 11]\n",
      " [ 1  0  2  0  2  4  1  3 55  5]\n",
      " [ 0  0  1  2  0  5  1  6  4 56]]\n",
      "Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>46</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>74</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>38</td>\n",
       "      <td>7</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>10</td>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>79</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>55</td>\n",
       "      <td>5</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>56</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>79</td>\n",
       "      <td>110</td>\n",
       "      <td>85</td>\n",
       "      <td>123</td>\n",
       "      <td>90</td>\n",
       "      <td>88</td>\n",
       "      <td>76</td>\n",
       "      <td>95</td>\n",
       "      <td>71</td>\n",
       "      <td>83</td>\n",
       "      <td>900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted   0    1   2    3   4   5   6   7   8   9  All\n",
       "True                                                    \n",
       "0          46   16   2   15   2   2   4   0   3   0   90\n",
       "1           4   74   3    8   1   0   1   0   0   0   91\n",
       "2           2    8  38    7  18   1   8   0   2   0   84\n",
       "3          12    5   8   63   1   1   6   0   2   0   98\n",
       "4           2    2  22   10  51   1  19   0   2   1  110\n",
       "5           0    0   0    4   0  63   1   7   0   7   82\n",
       "6          12    5   9   14  15   2  35   0   2   3   97\n",
       "7           0    0   0    0   0   9   0  79   1  11  100\n",
       "8           1    0   2    0   2   4   1   3  55   5   73\n",
       "9           0    0   1    2   0   5   1   6   4  56   75\n",
       "All        79  110  85  123  90  88  76  95  71  83  900"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a set of predictions for the test data\n",
    "y_pred = my_best_superlearner.predict(X_test)\n",
    "\n",
    "# Print performance details\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred) #normalize=True, \n",
    "                                            #sample_weight=None\n",
    "\n",
    "model_test_accuracy_comparisons[\"Best Setup Super Learner\"] = accuracy    \n",
    "    \n",
    "print(\"Accuracy: \" +  str(accuracy))\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(metrics.confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Print nicer confusion matrix\n",
    "print(\"Confusion Matrix\")\n",
    "pd.crosstab(np.array(y_test), y_pred, \n",
    "            rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training on train plus validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SuperLearnerClassifier(add_original_input=True,\n",
       "            base_classifiers=[DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=6,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=200,\n",
       "            min_weight_fraction_lea...b_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False), GaussianNB(priors=None)],\n",
       "            base_clsf_type=35, cv=3, probability_outputs=False,\n",
       "            shuffle=True,\n",
       "            stacked_layer_classifier=DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=6,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=200,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best'),\n",
       "            stratify=True, type_of_model=1)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "#Using the best setup found in Task 7 and adding original descriptive features at the stack layer.\n",
    "'''\n",
    "Best parameters set found on development set:\n",
    "{'base_clsf_type': 27, 'probability_outputs': 'True', 'type_of_model': 1}\n",
    "\n",
    "0.70428571428571429\n",
    "'''\n",
    "my_best_superlearner = SuperLearnerClassifier(type_of_model=1, base_clsf_type=35, probability_outputs=False, add_original_input=True)\n",
    "my_best_superlearner.fit(np.array(X_train_plus_valid),np.array(y_train_plus_valid))  #training best setup supe learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.69380952381\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.75      0.63      0.69       210\n",
      "          1       0.69      0.89      0.78       205\n",
      "          2       0.60      0.59      0.60       217\n",
      "          3       0.57      0.63      0.60       237\n",
      "          4       0.60      0.47      0.53       191\n",
      "          5       0.70      0.73      0.72       188\n",
      "          6       0.54      0.46      0.49       205\n",
      "          7       0.81      0.89      0.85       224\n",
      "          8       0.85      0.75      0.80       194\n",
      "          9       0.81      0.86      0.84       229\n",
      "\n",
      "avg / total       0.69      0.69      0.69      2100\n",
      "\n",
      "[[132  38   6  16   4   6   8   0   0   0]\n",
      " [  5 182   2  13   2   1   0   0   0   0]\n",
      " [  5  18 129  18  24   6  16   0   1   0]\n",
      " [ 15  16  16 150  10  13  17   0   0   0]\n",
      " [  1   4  45  15  90   3  28   0   4   1]\n",
      " [  0   0   1  10   1 138   4  28   0   6]\n",
      " [ 16   5  13  34  15  10  94   1  10   7]\n",
      " [  0   0   0   0   0  18   0 199   0   7]\n",
      " [  1   0   3   3   5   2   4   6 145  25]\n",
      " [  0   0   1   3   0   0   4  13  10 198]]\n",
      "Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>132</td>\n",
       "      <td>38</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>182</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>129</td>\n",
       "      <td>18</td>\n",
       "      <td>24</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>150</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>45</td>\n",
       "      <td>15</td>\n",
       "      <td>90</td>\n",
       "      <td>3</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>138</td>\n",
       "      <td>4</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>34</td>\n",
       "      <td>15</td>\n",
       "      <td>10</td>\n",
       "      <td>94</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>199</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>145</td>\n",
       "      <td>25</td>\n",
       "      <td>194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>198</td>\n",
       "      <td>229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>175</td>\n",
       "      <td>263</td>\n",
       "      <td>216</td>\n",
       "      <td>262</td>\n",
       "      <td>151</td>\n",
       "      <td>197</td>\n",
       "      <td>175</td>\n",
       "      <td>247</td>\n",
       "      <td>170</td>\n",
       "      <td>244</td>\n",
       "      <td>2100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted    0    1    2    3    4    5    6    7    8    9   All\n",
       "True                                                             \n",
       "0          132   38    6   16    4    6    8    0    0    0   210\n",
       "1            5  182    2   13    2    1    0    0    0    0   205\n",
       "2            5   18  129   18   24    6   16    0    1    0   217\n",
       "3           15   16   16  150   10   13   17    0    0    0   237\n",
       "4            1    4   45   15   90    3   28    0    4    1   191\n",
       "5            0    0    1   10    1  138    4   28    0    6   188\n",
       "6           16    5   13   34   15   10   94    1   10    7   205\n",
       "7            0    0    0    0    0   18    0  199    0    7   224\n",
       "8            1    0    3    3    5    2    4    6  145   25   194\n",
       "9            0    0    1    3    0    0    4   13   10  198   229\n",
       "All        175  263  216  262  151  197  175  247  170  244  2100"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a set of predictions for the training data\n",
    "y_pred = my_best_superlearner.predict(X_train_plus_valid)\n",
    "\n",
    "# Print performance details  y_train_plus_valid  \n",
    "accuracy = metrics.accuracy_score(y_train_plus_valid, y_pred) #normalize=True, \n",
    "                                                #sample_weight=None\n",
    "print(\"Accuracy: \" +  str(accuracy))\n",
    "print(metrics.classification_report(y_train_plus_valid, y_pred))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(metrics.confusion_matrix(y_train_plus_valid, y_pred))\n",
    "\n",
    "# Print nicer homemade confusion matrix\n",
    "print(\"Confusion Matrix\")\n",
    "pd.crosstab(np.array(y_train_plus_valid), y_pred, \n",
    "            rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.616666666667\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.62      0.50      0.55        90\n",
      "          1       0.65      0.77      0.71        91\n",
      "          2       0.43      0.49      0.46        84\n",
      "          3       0.50      0.57      0.53        98\n",
      "          4       0.56      0.42      0.48       110\n",
      "          5       0.76      0.79      0.77        82\n",
      "          6       0.39      0.38      0.39        97\n",
      "          7       0.84      0.83      0.83       100\n",
      "          8       0.82      0.77      0.79        73\n",
      "          9       0.67      0.75      0.71        75\n",
      "\n",
      "avg / total       0.62      0.62      0.61       900\n",
      "\n",
      "[[45 15  1 14  4  1  8  0  2  0]\n",
      " [ 3 70  3 10  0  0  5  0  0  0]\n",
      " [ 1  7 41  7 16  1 10  0  0  1]\n",
      " [10  8  8 56  1  3 10  0  2  0]\n",
      " [ 1  3 23  9 46  0 22  0  3  3]\n",
      " [ 0  0  0  3  0 65  2  8  0  4]\n",
      " [13  4 12 13 12  2 37  0  1  3]\n",
      " [ 0  0  0  0  0  6  0 83  0 11]\n",
      " [ 0  0  3  0  3  3  0  3 56  5]\n",
      " [ 0  0  4  1  0  5  0  5  4 56]]\n",
      "Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>45</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>70</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>41</td>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>9</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>65</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>83</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>56</td>\n",
       "      <td>5</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>56</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>73</td>\n",
       "      <td>107</td>\n",
       "      <td>95</td>\n",
       "      <td>113</td>\n",
       "      <td>82</td>\n",
       "      <td>86</td>\n",
       "      <td>94</td>\n",
       "      <td>99</td>\n",
       "      <td>68</td>\n",
       "      <td>83</td>\n",
       "      <td>900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted   0    1   2    3   4   5   6   7   8   9  All\n",
       "True                                                    \n",
       "0          45   15   1   14   4   1   8   0   2   0   90\n",
       "1           3   70   3   10   0   0   5   0   0   0   91\n",
       "2           1    7  41    7  16   1  10   0   0   1   84\n",
       "3          10    8   8   56   1   3  10   0   2   0   98\n",
       "4           1    3  23    9  46   0  22   0   3   3  110\n",
       "5           0    0   0    3   0  65   2   8   0   4   82\n",
       "6          13    4  12   13  12   2  37   0   1   3   97\n",
       "7           0    0   0    0   0   6   0  83   0  11  100\n",
       "8           0    0   3    0   3   3   0   3  56   5   73\n",
       "9           0    0   4    1   0   5   0   5   4  56   75\n",
       "All        73  107  95  113  82  86  94  99  68  83  900"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a set of predictions for the test data\n",
    "y_pred = my_best_superlearner.predict(X_test)\n",
    "\n",
    "# Print performance details\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred) #normalize=True, \n",
    "                                            #sample_weight=None\n",
    "\n",
    "model_test_accuracy_comparisons[\"Best Setup Super Learner\"] = accuracy    \n",
    "    \n",
    "print(\"Accuracy: \" +  str(accuracy))\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(metrics.confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Print nicer confusion matrix\n",
    "print(\"Confusion Matrix\")\n",
    "pd.crosstab(np.array(y_test), y_pred, \n",
    "            rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training on entire MNIST training data and evaluating on the MNIST test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SuperLearnerClassifier(add_original_input=True,\n",
       "            base_classifiers=[DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=6,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=200,\n",
       "            min_weight_fraction_lea...b_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False), GaussianNB(priors=None)],\n",
       "            base_clsf_type=35, cv=3, probability_outputs=False,\n",
       "            shuffle=True,\n",
       "            stacked_layer_classifier=DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=6,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=200,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best'),\n",
       "            stratify=True, type_of_model=1)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "#Using the best setup found in Task 7 and adding original descriptive features at the stack layer.\n",
    "'''\n",
    "Best parameters set found on development set:\n",
    "{'base_clsf_type': 27, 'probability_outputs': 'True', 'type_of_model': 1}\n",
    "\n",
    "0.70428571428571429\n",
    "'''\n",
    "my_best_superlearner = SuperLearnerClassifier(type_of_model=1, base_clsf_type=35, probability_outputs=False, add_original_input=True)\n",
    "my_best_superlearner.fit(np.array(X),np.array(Y))  #training best setup supe learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.694\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.70      0.62      0.65       300\n",
      "          1       0.70      0.90      0.79       296\n",
      "          2       0.64      0.60      0.62       301\n",
      "          3       0.59      0.60      0.59       335\n",
      "          4       0.57      0.54      0.56       301\n",
      "          5       0.71      0.76      0.73       270\n",
      "          6       0.55      0.48      0.52       302\n",
      "          7       0.81      0.86      0.84       324\n",
      "          8       0.90      0.76      0.82       267\n",
      "          9       0.79      0.83      0.81       304\n",
      "\n",
      "avg / total       0.69      0.69      0.69      3000\n",
      "\n",
      "[[185  49  10  32   6   5  13   0   0   0]\n",
      " [  9 266   3  14   2   0   2   0   0   0]\n",
      " [  6  22 182  22  43   6  17   0   1   2]\n",
      " [ 31  29  13 201  18  17  26   0   0   0]\n",
      " [  3   6  55  23 164   5  40   0   5   0]\n",
      " [  0   0   2  14   1 205   5  34   0   9]\n",
      " [ 31   7  16  31  43  12 146   1   8   7]\n",
      " [  0   0   0   0   0  25   0 280   0  19]\n",
      " [  0   0   2   3  11   8   5   8 202  28]\n",
      " [  0   0   2   2   1   7  10  23   8 251]]\n",
      "Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>185</td>\n",
       "      <td>49</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>266</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>22</td>\n",
       "      <td>182</td>\n",
       "      <td>22</td>\n",
       "      <td>43</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31</td>\n",
       "      <td>29</td>\n",
       "      <td>13</td>\n",
       "      <td>201</td>\n",
       "      <td>18</td>\n",
       "      <td>17</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>55</td>\n",
       "      <td>23</td>\n",
       "      <td>164</td>\n",
       "      <td>5</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>205</td>\n",
       "      <td>5</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>31</td>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "      <td>31</td>\n",
       "      <td>43</td>\n",
       "      <td>12</td>\n",
       "      <td>146</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>280</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>202</td>\n",
       "      <td>28</td>\n",
       "      <td>267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>23</td>\n",
       "      <td>8</td>\n",
       "      <td>251</td>\n",
       "      <td>304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>265</td>\n",
       "      <td>379</td>\n",
       "      <td>285</td>\n",
       "      <td>342</td>\n",
       "      <td>289</td>\n",
       "      <td>290</td>\n",
       "      <td>264</td>\n",
       "      <td>346</td>\n",
       "      <td>224</td>\n",
       "      <td>316</td>\n",
       "      <td>3000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted    0    1    2    3    4    5    6    7    8    9   All\n",
       "True                                                             \n",
       "0          185   49   10   32    6    5   13    0    0    0   300\n",
       "1            9  266    3   14    2    0    2    0    0    0   296\n",
       "2            6   22  182   22   43    6   17    0    1    2   301\n",
       "3           31   29   13  201   18   17   26    0    0    0   335\n",
       "4            3    6   55   23  164    5   40    0    5    0   301\n",
       "5            0    0    2   14    1  205    5   34    0    9   270\n",
       "6           31    7   16   31   43   12  146    1    8    7   302\n",
       "7            0    0    0    0    0   25    0  280    0   19   324\n",
       "8            0    0    2    3   11    8    5    8  202   28   267\n",
       "9            0    0    2    2    1    7   10   23    8  251   304\n",
       "All        265  379  285  342  289  290  264  346  224  316  3000"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a set of predictions for the test data\n",
    "y_pred = my_best_superlearner.predict(X)\n",
    "\n",
    "# Print performance details\n",
    "accuracy = metrics.accuracy_score(Y, y_pred) #normalize=True, \n",
    "                                            #sample_weight=None\n",
    "\n",
    "model_test_accuracy_comparisons[\"Best Setup Super Learner\"] = accuracy    \n",
    "    \n",
    "print(\"Accuracy: \" +  str(accuracy))\n",
    "print(metrics.classification_report(Y, y_pred))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(metrics.confusion_matrix(Y, y_pred))\n",
    "\n",
    "# Print nicer confusion matrix\n",
    "print(\"Confusion Matrix\")\n",
    "pd.crosstab(np.array(Y), y_pred, \n",
    "            rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6433\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.69      0.60      0.64      1000\n",
      "          1       0.69      0.90      0.78      1000\n",
      "          2       0.52      0.55      0.53      1000\n",
      "          3       0.55      0.62      0.58      1000\n",
      "          4       0.53      0.48      0.50      1000\n",
      "          5       0.65      0.69      0.67      1000\n",
      "          6       0.45      0.34      0.39      1000\n",
      "          7       0.75      0.83      0.79      1000\n",
      "          8       0.88      0.68      0.76      1000\n",
      "          9       0.73      0.75      0.74      1000\n",
      "\n",
      "avg / total       0.64      0.64      0.64     10000\n",
      "\n",
      "[[599 123  31 109  28  20  66   1  20   3]\n",
      " [ 25 900  18  44   6   4   3   0   0   0]\n",
      " [ 28  83 547  51 168  15  90   0   9   9]\n",
      " [ 64 109  43 624  27  48  80   0   3   2]\n",
      " [  9  26 235  80 480  12 133   0  18   7]\n",
      " [  8   4   1  49   3 688  16 156   3  72]\n",
      " [126  63 121 142 145  46 340   0  12   5]\n",
      " [  0   0   0   0   0 104   0 828   5  63]\n",
      " [  6   1  24  24  47  54  17  39 675 113]\n",
      " [  3   0  38  17   6  64  18  78  24 752]]\n",
      "Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>599</td>\n",
       "      <td>123</td>\n",
       "      <td>31</td>\n",
       "      <td>109</td>\n",
       "      <td>28</td>\n",
       "      <td>20</td>\n",
       "      <td>66</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25</td>\n",
       "      <td>900</td>\n",
       "      <td>18</td>\n",
       "      <td>44</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>83</td>\n",
       "      <td>547</td>\n",
       "      <td>51</td>\n",
       "      <td>168</td>\n",
       "      <td>15</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>64</td>\n",
       "      <td>109</td>\n",
       "      <td>43</td>\n",
       "      <td>624</td>\n",
       "      <td>27</td>\n",
       "      <td>48</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>26</td>\n",
       "      <td>235</td>\n",
       "      <td>80</td>\n",
       "      <td>480</td>\n",
       "      <td>12</td>\n",
       "      <td>133</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>7</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "      <td>3</td>\n",
       "      <td>688</td>\n",
       "      <td>16</td>\n",
       "      <td>156</td>\n",
       "      <td>3</td>\n",
       "      <td>72</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>126</td>\n",
       "      <td>63</td>\n",
       "      <td>121</td>\n",
       "      <td>142</td>\n",
       "      <td>145</td>\n",
       "      <td>46</td>\n",
       "      <td>340</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>104</td>\n",
       "      <td>0</td>\n",
       "      <td>828</td>\n",
       "      <td>5</td>\n",
       "      <td>63</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>47</td>\n",
       "      <td>54</td>\n",
       "      <td>17</td>\n",
       "      <td>39</td>\n",
       "      <td>675</td>\n",
       "      <td>113</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>64</td>\n",
       "      <td>18</td>\n",
       "      <td>78</td>\n",
       "      <td>24</td>\n",
       "      <td>752</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>868</td>\n",
       "      <td>1309</td>\n",
       "      <td>1058</td>\n",
       "      <td>1140</td>\n",
       "      <td>910</td>\n",
       "      <td>1055</td>\n",
       "      <td>763</td>\n",
       "      <td>1102</td>\n",
       "      <td>769</td>\n",
       "      <td>1026</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted    0     1     2     3    4     5    6     7    8     9    All\n",
       "True                                                                    \n",
       "0          599   123    31   109   28    20   66     1   20     3   1000\n",
       "1           25   900    18    44    6     4    3     0    0     0   1000\n",
       "2           28    83   547    51  168    15   90     0    9     9   1000\n",
       "3           64   109    43   624   27    48   80     0    3     2   1000\n",
       "4            9    26   235    80  480    12  133     0   18     7   1000\n",
       "5            8     4     1    49    3   688   16   156    3    72   1000\n",
       "6          126    63   121   142  145    46  340     0   12     5   1000\n",
       "7            0     0     0     0    0   104    0   828    5    63   1000\n",
       "8            6     1    24    24   47    54   17    39  675   113   1000\n",
       "9            3     0    38    17    6    64   18    78   24   752   1000\n",
       "All        868  1309  1058  1140  910  1055  763  1102  769  1026  10000"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a set of predictions for the test data\n",
    "y_pred = my_best_superlearner.predict(test_X)\n",
    "\n",
    "# Print performance details\n",
    "accuracy = metrics.accuracy_score(test_Y, y_pred) #normalize=True, \n",
    "                                            #sample_weight=None\n",
    "\n",
    "model_test_accuracy_comparisons[\"Best Setup Super Learner\"] = accuracy    \n",
    "    \n",
    "print(\"Accuracy: \" +  str(accuracy))\n",
    "print(metrics.classification_report(test_Y, y_pred))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(metrics.confusion_matrix(test_Y, y_pred))\n",
    "\n",
    "# Print nicer confusion matrix\n",
    "print(\"Confusion Matrix\")\n",
    "pd.crosstab(np.array(test_Y), y_pred, \n",
    "            rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perfroming a 10-fold cross validation experiment to evaluate the performance of the SuperLearnerClassifier \n",
    "#### with the best setup from the task 7 and added original input to the input at the stack layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores = cross_val_score(my_best_superlearner, np.array(X_train_plus_valid), np.array(y_train_plus_valid), cv=10, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.61214953  0.62441315  0.61502347  0.657277    0.62085308  0.62679426\n",
      "  0.66985646  0.68115942  0.67961165  0.61463415]\n",
      "0.640177216312\n"
     ]
    }
   ],
   "source": [
    "print(scores)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Ensemble Model (Task 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform an analysis to investigate the strength of the base estimators and the strengths of the correlations between them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Strenghts of correlations between base estimators\n",
    "\n",
    "Using Pearson correlation to measure diversity/correlation. We want base estimators to be weakly correlated/diverse, hence we want pearson correlation to be close to 0\n",
    "\n",
    "Note: Pearson correlation coefficient is a measure of the linear correlation between two variables X and Y.\n",
    "\n",
    "The Pearson correlation coefficient, r, can take a range of values from +1 to -1. A value of 0 indicates that there is no association between the two variables. A value greater than 0 indicates a positive association; that is, as the value of one variable increases, so does the value of the other variable.\n",
    "\n",
    "The stronger the association of the two variables, the closer the Pearson correlation coefficient, r, will be to either +1 or -1 depending on whether the relationship is positive or negative, respectively. \n",
    "\n",
    "Achieving a value of +1 or -1 means that all your data points are included on the line of best fit â€“ there are no data points that show any variation away from this line. Values for r between +1 and -1 (for example, r = 0.8 or -0.4) indicate that there is variation around the line of best fit. The closer the value of r to 0 the greater the variation around the line of best fit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_classifiers=[DecisionTreeClassifier(criterion=\"entropy\", min_samples_split = 200, max_depth=6),\n",
    "            LogisticRegression(C=1.8, max_iter=1000, multi_class='ovr', solver= 'liblinear'),\n",
    "            svm.SVC(kernel='linear', C=100, probability=True),\n",
    "            KNeighborsClassifier(n_neighbors=6) ,\n",
    "            RandomForestClassifier(criterion='gini',max_depth=100,n_estimators=10),\n",
    "            GaussianNB()]  \n",
    "\n",
    "#Training the base classifiers\n",
    "for clf in base_classifiers:\n",
    "    clf.fit(X,Y)\n",
    "        \n",
    "#The generation of the stacked layer training set\n",
    "clfs_outputs = np.column_stack([clf.predict(X) for clf in base_classifiers])\n",
    "\n",
    "#print(np.transpose(clfs_outputs))               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating a Pearson correlation coefficient and the p-value to test non-correlation:\n",
      "\n",
      "Base model 1 and base model 2 : (0.81940498253004379, 0.0)\n",
      "Base model 1 and base model 3 : (0.81130830527324016, 0.0)\n",
      "Base model 1 and base model 4 : (0.79868155928073614, 0.0)\n",
      "Base model 1 and base model 5 : (0.76708317158918926, 0.0)\n",
      "Base model 1 and base model 6 : (0.84207836067546082, 0.0)\n",
      "Base model 2 and base model 3 : (0.88728677868888728, 0.0)\n",
      "Base model 2 and base model 4 : (0.83600372586948324, 0.0)\n",
      "Base model 2 and base model 5 : (0.7888351857611553, 0.0)\n",
      "Base model 2 and base model 6 : (0.84047731106177737, 0.0)\n",
      "Base model 3 and base model 4 : (0.85438383309037236, 0.0)\n",
      "Base model 3 and base model 5 : (0.82058598724362786, 0.0)\n",
      "Base model 3 and base model 6 : (0.82049675343658912, 0.0)\n",
      "Base model 4 and base model 5 : (0.84121308847880816, 0.0)\n",
      "Base model 4 and base model 6 : (0.82895585641053005, 0.0)\n",
      "Base model 5 and base model 6 : (0.77181380976840241, 0.0)\n"
     ]
    }
   ],
   "source": [
    "print('Calculating a Pearson correlation coefficient and the p-value to test non-correlation:\\n')\n",
    "#The p-value roughly indicates the probability of an uncorrelated system producing datasets that have a Pearson correlation at least as extreme as the one computed from these datasets. The p-values are not entirely reliable but are probably reasonable for datasets larger than 500 or so.\n",
    "for i in range(5):\n",
    "    for j in range(i+1,6):\n",
    "        print('Base model',i+1,'and base model', j+1,':',scipy.stats.pearsonr(np.transpose(clfs_outputs)[i], np.transpose(clfs_outputs)[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.819405</td>\n",
       "      <td>0.811308</td>\n",
       "      <td>0.798682</td>\n",
       "      <td>0.767083</td>\n",
       "      <td>0.842078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.819405</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.887287</td>\n",
       "      <td>0.836004</td>\n",
       "      <td>0.788835</td>\n",
       "      <td>0.840477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.811308</td>\n",
       "      <td>0.887287</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.854384</td>\n",
       "      <td>0.820586</td>\n",
       "      <td>0.820497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.798682</td>\n",
       "      <td>0.836004</td>\n",
       "      <td>0.854384</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.841213</td>\n",
       "      <td>0.828956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.767083</td>\n",
       "      <td>0.788835</td>\n",
       "      <td>0.820586</td>\n",
       "      <td>0.841213</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.771814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.842078</td>\n",
       "      <td>0.840477</td>\n",
       "      <td>0.820497</td>\n",
       "      <td>0.828956</td>\n",
       "      <td>0.771814</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5\n",
       "0  1.000000  0.819405  0.811308  0.798682  0.767083  0.842078\n",
       "1  0.819405  1.000000  0.887287  0.836004  0.788835  0.840477\n",
       "2  0.811308  0.887287  1.000000  0.854384  0.820586  0.820497\n",
       "3  0.798682  0.836004  0.854384  1.000000  0.841213  0.828956\n",
       "4  0.767083  0.788835  0.820586  0.841213  1.000000  0.771814\n",
       "5  0.842078  0.840477  0.820497  0.828956  0.771814  1.000000"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#another way\n",
    "df=pd.DataFrame(data=clfs_outputs)\n",
    "df.corr(method='pearson', min_periods=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 3 3 ..., 5 4 4]\n",
      " [1 0 6 ..., 5 2 4]\n",
      " [1 0 6 ..., 5 2 4]\n",
      " [1 3 2 ..., 5 4 4]\n",
      " [1 3 2 ..., 5 2 4]\n",
      " [1 0 3 ..., 7 4 4]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 1, 2, 3, 4, 5)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clfs_outputs_tr=np.transpose(clfs_outputs)\n",
    "print(clfs_outputs_tr)\n",
    "\n",
    "mat = np.array(clfs_outputs_tr)  \n",
    "_, inds = sympy.Matrix(mat).T.rref()   #linear independant rows by using: sympy.Matrix.rref:\n",
    "inds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "For classification (such is our case: 10 labels for predictors), classification accuracy is appropriate measure\n",
    "\n",
    "Some of the appropriate measures for classification are: accuracy, geometric mean, precision, recall, ROC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Performance of your model 0 :\n",
      "Accuracy score: 0.621333333333\n",
      "Accuracy: 0.616666666667\n",
      "Classification Report:              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.46      0.57      0.51       287\n",
      "          1       0.92      0.69      0.79       321\n",
      "          2       0.43      0.66      0.52       290\n",
      "          3       0.44      0.66      0.53       293\n",
      "          4       0.60      0.60      0.60       292\n",
      "          5       0.82      0.69      0.75       326\n",
      "          6       0.00      0.00      0.00       286\n",
      "          7       0.67      0.82      0.74       293\n",
      "          8       0.73      0.79      0.76       303\n",
      "          9       0.77      0.68      0.72       309\n",
      "\n",
      "avg / total       0.59      0.62      0.60      3000\n",
      "\n",
      "Confusion Matrix\n",
      "Predicted    0    1    2    3    4    5    7    8    9   All\n",
      "True                                                        \n",
      "0          164    6   26   70    4    2    1   14    0   287\n",
      "1           24  222   15   58    2    0    0    0    0   321\n",
      "2           31    2  190   18   43    0    0    5    1   290\n",
      "3           39   11   35  194    0    0    1    8    5   293\n",
      "4           12    0   64   27  176    0    0    8    5   292\n",
      "5            5    0    0    5    1  226   64    9   16   326\n",
      "6           73    0   86   45   63    0    3    9    7   286\n",
      "7            0    0    0    0    0   24  241   10   18   293\n",
      "8            2    0   12   18    1    5   13  240   12   303\n",
      "9            5    0    9    4    2   17   36   25  211   309\n",
      "All        355  241  437  439  292  274  359  328  275  3000\n",
      "\n",
      " Performance of your model 1 :\n",
      "Accuracy score: 0.641\n",
      "Accuracy: 0.616666666667\n",
      "Classification Report:              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.70      0.66      0.68       287\n",
      "          1       0.70      0.90      0.79       321\n",
      "          2       0.52      0.52      0.52       290\n",
      "          3       0.53      0.56      0.54       293\n",
      "          4       0.52      0.51      0.52       292\n",
      "          5       0.62      0.70      0.66       326\n",
      "          6       0.39      0.23      0.29       286\n",
      "          7       0.77      0.78      0.77       293\n",
      "          8       0.78      0.73      0.75       303\n",
      "          9       0.76      0.75      0.76       309\n",
      "\n",
      "avg / total       0.63      0.64      0.63      3000\n",
      "\n",
      "Confusion Matrix\n",
      "Predicted    0    1    2    3    4    5    6    7    8    9   All\n",
      "True                                                             \n",
      "0          190   35    8   31    3    2   10    0    8    0   287\n",
      "1            3  290    4   15    3    6    0    0    0    0   321\n",
      "2            4   26  152   23   44    5   28    0    4    4   290\n",
      "3           29   28   18  164   15   21   14    0    3    1   293\n",
      "4            0   10   53   14  150    5   27    0   30    3   292\n",
      "5            3    0    1   17    0  228    5   42    3   27   326\n",
      "6           42   25   33   34   57   18   65    1    7    4   286\n",
      "7            0    0    0    0    0   45    0  229    0   19   293\n",
      "8            0    0    5    9   16   21    5    8  222   17   303\n",
      "9            0    0   17    5    1   14   11   19    9  233   309\n",
      "All        271  414  291  312  289  365  165  299  286  308  3000\n",
      "\n",
      " Performance of your model 2 :\n",
      "Accuracy score: 0.729333333333\n",
      "Accuracy: 0.616666666667\n",
      "Classification Report:              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.75      0.73      0.74       287\n",
      "          1       0.85      0.90      0.88       321\n",
      "          2       0.61      0.59      0.60       290\n",
      "          3       0.60      0.73      0.66       293\n",
      "          4       0.70      0.64      0.67       292\n",
      "          5       0.78      0.78      0.78       326\n",
      "          6       0.48      0.42      0.45       286\n",
      "          7       0.79      0.83      0.81       293\n",
      "          8       0.88      0.81      0.85       303\n",
      "          9       0.80      0.82      0.81       309\n",
      "\n",
      "avg / total       0.73      0.73      0.73      3000\n",
      "\n",
      "Confusion Matrix\n",
      "Predicted    0    1    2    3    4    5    6    7    8    9   All\n",
      "True                                                             \n",
      "0          210   17    4   31    2    1   18    0    3    1   287\n",
      "1            1  290    4   21    2    0    3    0    0    0   321\n",
      "2            5    6  172   19   30    1   47    0    4    6   290\n",
      "3           23   12   10  215    8    9   11    0    0    5   293\n",
      "4            0    2   31   26  187    1   33    0    9    3   292\n",
      "5            0    0    2    7    0  253    3   41    3   17   326\n",
      "6           34   14   40   38   33    0  119    0    5    3   286\n",
      "7            0    0    0    0    0   34    0  244    0   15   293\n",
      "8            4    0    6    3    4   12    6   10  246   12   303\n",
      "9            2    0   13    0    1   12    8   13    8  252   309\n",
      "All        279  341  282  360  267  323  248  308  278  314  3000\n",
      "\n",
      " Performance of your model 3 :\n",
      "Accuracy score: 0.754\n",
      "Accuracy: 0.616666666667\n",
      "Classification Report:              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.73      0.74      0.73       287\n",
      "          1       0.76      0.95      0.85       321\n",
      "          2       0.65      0.69      0.67       290\n",
      "          3       0.64      0.73      0.68       293\n",
      "          4       0.67      0.68      0.67       292\n",
      "          5       0.87      0.75      0.80       326\n",
      "          6       0.61      0.49      0.54       286\n",
      "          7       0.86      0.87      0.87       293\n",
      "          8       0.95      0.80      0.86       303\n",
      "          9       0.81      0.83      0.82       309\n",
      "\n",
      "avg / total       0.76      0.75      0.75      3000\n",
      "\n",
      "Confusion Matrix\n",
      "Predicted    0    1    2    3    4    5    6    7    8    9   All\n",
      "True                                                             \n",
      "0          211   23    8   24    3    2   15    0    1    0   287\n",
      "1            3  305    2    7    1    0    3    0    0    0   321\n",
      "2            6   15  200   12   35    0   21    0    0    1   290\n",
      "3           16   32    8  213   10    0   13    0    1    0   293\n",
      "4            0    7   37   19  198    0   27    0    3    1   292\n",
      "5            4    0    4   24    2  244    2   23    3   20   326\n",
      "6           43   17   38   16   28    1  139    0    2    2   286\n",
      "7            0    0    0    0    1   16    0  256    1   19   293\n",
      "8            5    0    9    8    8    4    4    8  241   16   303\n",
      "9            0    1    4    8    9   14    4   11    3  255   309\n",
      "All        288  400  310  331  295  281  228  298  255  314  3000\n",
      "\n",
      " Performance of your model 4 :\n",
      "Accuracy score: 0.988\n",
      "Accuracy: 0.616666666667\n",
      "Classification Report:              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.99      0.98       287\n",
      "          1       0.99      1.00      0.99       321\n",
      "          2       0.97      0.99      0.98       290\n",
      "          3       0.98      0.98      0.98       293\n",
      "          4       0.99      0.98      0.99       292\n",
      "          5       0.99      1.00      1.00       326\n",
      "          6       1.00      0.96      0.98       286\n",
      "          7       1.00      1.00      1.00       293\n",
      "          8       0.99      1.00      0.99       303\n",
      "          9       1.00      0.98      0.99       309\n",
      "\n",
      "avg / total       0.99      0.99      0.99      3000\n",
      "\n",
      "Confusion Matrix\n",
      "Predicted    0    1    2    3    4    5    6    7    8    9   All\n",
      "True                                                             \n",
      "0          284    0    0    1    0    1    1    0    0    0   287\n",
      "1            1  320    0    0    0    0    0    0    0    0   321\n",
      "2            1    0  288    0    0    0    0    0    1    0   290\n",
      "3            2    3    1  287    0    0    0    0    0    0   293\n",
      "4            0    0    3    2  287    0    0    0    0    0   292\n",
      "5            0    0    0    0    0  326    0    0    0    0   326\n",
      "6            4    0    4    1    3    0  274    0    0    0   286\n",
      "7            0    0    0    0    0    1    0  292    0    0   293\n",
      "8            0    0    0    0    0    0    0    0  303    0   303\n",
      "9            0    0    1    1    0    1    0    0    3  303   309\n",
      "All        292  323  297  292  290  329  275  292  307  303  3000\n",
      "\n",
      " Performance of your model 5 :\n",
      "Accuracy score: 0.511666666667\n",
      "Accuracy: 0.616666666667\n",
      "Classification Report:              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.64      0.44      0.52       287\n",
      "          1       0.48      0.90      0.63       321\n",
      "          2       0.55      0.20      0.30       290\n",
      "          3       0.34      0.38      0.35       293\n",
      "          4       0.41      0.76      0.53       292\n",
      "          5       0.47      0.17      0.25       326\n",
      "          6       0.17      0.01      0.03       286\n",
      "          7       0.45      0.98      0.62       293\n",
      "          8       0.93      0.74      0.82       303\n",
      "          9       0.78      0.50      0.61       309\n",
      "\n",
      "avg / total       0.52      0.51      0.47      3000\n",
      "\n",
      "Confusion Matrix\n",
      "Predicted    0    1    2    3    4    5   6    7    8    9   All\n",
      "True                                                            \n",
      "0          127   71   13   34   24    6   9    1    0    2   287\n",
      "1           15  289    3    8    2    4   0    0    0    0   321\n",
      "2            2   56   59   56  108    3   0    0    3    3   290\n",
      "3           33   87    0  110   39   18   5    0    0    1   293\n",
      "4            1   21    4   29  223    4   0    0    6    4   292\n",
      "5            0    1    0   19    1   55   3  235    2   10   326\n",
      "6           18   71   16   51  107   11   4    1    3    4   286\n",
      "7            0    0    0    0    0    0   0  288    0    5   293\n",
      "8            1    0   11   12    9    9   0   21  224   16   303\n",
      "9            1    1    1    8   33    8   3   94    4  156   309\n",
      "All        198  597  107  327  546  118  24  640  242  201  3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aidasehic/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "#Y - true labels\n",
    "#clfs_outputs_tr[i] - predicted labels by model i\n",
    "#print(len(clfs_outputs_tr[0]), len(Y))\n",
    "\n",
    "\n",
    "for i in range(6):\n",
    "    print('\\n Performance of your model',i,':')\n",
    "    print('Accuracy score:', metrics.accuracy_score(Y, clfs_outputs_tr[i]))\n",
    "    print(\"Accuracy: \" + str(accuracy))\n",
    "    print('Classification Report:', metrics.classification_report(Y, clfs_outputs_tr[i]))\n",
    "    #print('Confusion Matrix:', metrics.confusion_matrix(Y, clfs_outputs_tr[i]))\n",
    "    # Print nicer confusion matrix\n",
    "    print(\"Confusion Matrix\")\n",
    "    print(pd.crosstab(np.array(Y),clfs_outputs_tr[i], rownames=['True'], \n",
    "                        colnames=['Predicted'], margins=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
